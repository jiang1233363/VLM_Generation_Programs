#!/usr/bin/env python3
"""
è§†è§‰æ¨¡å‹è¾¹ç•Œè¯„æµ‹æ¡†æ¶
ä½¿ç”¨è‰²ç›²æµ‹è¯•æ•°æ®é›†è¯„ä¼°è§†è§‰æ¨¡å‹çš„è‰²å½©æ„ŸçŸ¥è¾¹ç•Œ
"""

import json
import numpy as np
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Callable
import cv2

class ColorBlindnessEvaluator:
    """è‰²ç›²æµ‹è¯•æ•°æ®é›†è¯„æµ‹å™¨"""
    
    def __init__(self, dataset_path="data/gradients", metadata_path="metadata/final_dataset.json"):
        self.dataset_path = Path(dataset_path)
        self.metadata_path = Path(metadata_path)
        
        # åŠ è½½æ•°æ®é›†å…ƒæ•°æ®
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        # è‰²ç›²ç±»å‹
        self.colorblind_types = ['protanopia', 'deuteranopia', 'tritanopia']
        
        # è¯„æµ‹ç»“æœå­˜å‚¨
        self.evaluation_results = {}
        
    def evaluate_model_boundary(self, model_predict_func: Callable, 
                               confidence_threshold: float = 0.5) -> Dict:
        """
        è¯„æµ‹æ¨¡å‹çš„è§†è§‰è¾¹ç•Œ
        
        Args:
            model_predict_func: æ¨¡å‹é¢„æµ‹å‡½æ•°ï¼Œè¾“å…¥å›¾åƒè·¯å¾„ï¼Œè¿”å›(é¢„æµ‹ç»“æœ, ç½®ä¿¡åº¦)
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            è¯„æµ‹ç»“æœå­—å…¸
        """
        print("ğŸ” å¼€å§‹è¯„æµ‹è§†è§‰æ¨¡å‹è¾¹ç•Œ...")
        
        results = {
            "overall_metrics": {},
            "per_colorblind_type": {},
            "per_image_analysis": {},
            "boundary_analysis": {}
        }
        
        total_sequences = len(self.metadata["images"])
        
        for i, image_meta in enumerate(self.metadata["images"]):
            print(f"è¯„æµ‹å›¾åƒ {i+1}/{total_sequences}: {image_meta['base_image']}")
            
            base_image = image_meta['base_image']
            expected_answer = self.extract_expected_answer(base_image)
            
            image_results = {
                "base_image": base_image,
                "expected_answer": expected_answer,
                "colorblind_results": {}
            }
            
            for cb_type in self.colorblind_types:
                if cb_type in image_meta["colorblind_variants"]:
                    cb_result = self.evaluate_colorblind_sequence(
                        image_meta["colorblind_variants"][cb_type],
                        model_predict_func,
                        expected_answer,
                        confidence_threshold
                    )
                    image_results["colorblind_results"][cb_type] = cb_result
            
            results["per_image_analysis"][base_image] = image_results
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        results["overall_metrics"] = self.calculate_overall_metrics(results["per_image_analysis"])
        results["per_colorblind_type"] = self.calculate_per_type_metrics(results["per_image_analysis"])
        results["boundary_analysis"] = self.analyze_boundaries(results["per_image_analysis"])
        
        return results
    
    def evaluate_colorblind_sequence(self, variant_meta: Dict, 
                                   model_predict_func: Callable,
                                   expected_answer: str,
                                   confidence_threshold: float) -> Dict:
        """è¯„æµ‹å•ä¸ªè‰²ç›²ç±»å‹çš„æ¢¯åº¦åºåˆ—"""
        
        gradient_files = variant_meta["generated_files"]
        colorblind_type = variant_meta["colorblind_type"]
        
        sequence_results = {
            "colorblind_type": colorblind_type,
            "total_steps": len(gradient_files),
            "predictions": [],
            "accuracy_curve": [],
            "confidence_curve": [],
            "failure_threshold": None,
            "recovery_threshold": None
        }
        
        print(f"  è¯„æµ‹ {colorblind_type} åºåˆ—...")
        
        for step, image_path in enumerate(gradient_files):
            severity = step / (len(gradient_files) - 1)  # 0.0 åˆ° 1.0
            
            try:
                # è°ƒç”¨æ¨¡å‹é¢„æµ‹
                prediction, confidence = model_predict_func(image_path)
                
                # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                is_correct = self.is_prediction_correct(prediction, expected_answer)
                is_confident = confidence >= confidence_threshold
                
                step_result = {
                    "step": step,
                    "severity": severity,
                    "image_path": image_path,
                    "prediction": prediction,
                    "confidence": confidence,
                    "is_correct": is_correct,
                    "is_confident": is_confident,
                    "success": is_correct and is_confident
                }
                
                sequence_results["predictions"].append(step_result)
                sequence_results["accuracy_curve"].append(1.0 if is_correct else 0.0)
                sequence_results["confidence_curve"].append(confidence)
                
            except Exception as e:
                print(f"    âœ— æ­¥éª¤ {step} é¢„æµ‹å¤±è´¥: {e}")
                sequence_results["predictions"].append({
                    "step": step,
                    "severity": severity,
                    "error": str(e)
                })
        
        # åˆ†æå¤±è´¥å’Œæ¢å¤é˜ˆå€¼
        sequence_results["failure_threshold"] = self.find_failure_threshold(sequence_results["predictions"])
        sequence_results["recovery_threshold"] = self.find_recovery_threshold(sequence_results["predictions"])
        
        return sequence_results
    
    def find_failure_threshold(self, predictions: List[Dict]) -> float:
        """æ‰¾åˆ°æ¨¡å‹å¼€å§‹å¤±è´¥çš„è‰²ç›²ä¸¥é‡ç¨‹åº¦é˜ˆå€¼"""
        for pred in predictions:
            if "success" in pred and not pred["success"]:
                return pred["severity"]
        return 1.0  # å¦‚æœå§‹ç»ˆæˆåŠŸ
    
    def find_recovery_threshold(self, predictions: List[Dict]) -> float:
        """æ‰¾åˆ°æ¨¡å‹ä»å¤±è´¥ä¸­æ¢å¤çš„é˜ˆå€¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"""
        failed = False
        for pred in predictions:
            if "success" in pred:
                if not pred["success"]:
                    failed = True
                elif failed and pred["success"]:
                    return pred["severity"]
        return None  # æ²¡æœ‰æ¢å¤
    
    def calculate_overall_metrics(self, per_image_analysis: Dict) -> Dict:
        """è®¡ç®—æ€»ä½“è¯„æµ‹æŒ‡æ ‡"""
        
        all_predictions = []
        for image_results in per_image_analysis.values():
            for cb_type, cb_results in image_results["colorblind_results"].items():
                all_predictions.extend(cb_results["predictions"])
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆé¢„æµ‹
        valid_predictions = [p for p in all_predictions if "success" in p]
        
        if not valid_predictions:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆé¢„æµ‹"}
        
        total_predictions = len(valid_predictions)
        successful_predictions = sum(1 for p in valid_predictions if p["success"])
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„è®¡ç®—å‡†ç¡®ç‡
        severity_bins = np.linspace(0, 1, 11)  # 10ä¸ªåŒºé—´
        severity_accuracy = []
        
        for i in range(len(severity_bins) - 1):
            bin_start, bin_end = severity_bins[i], severity_bins[i + 1]
            bin_predictions = [p for p in valid_predictions 
                             if bin_start <= p["severity"] < bin_end]
            
            if bin_predictions:
                bin_accuracy = sum(1 for p in bin_predictions if p["success"]) / len(bin_predictions)
                severity_accuracy.append({
                    "severity_range": f"{bin_start:.1f}-{bin_end:.1f}",
                    "accuracy": bin_accuracy,
                    "sample_count": len(bin_predictions)
                })
        
        return {
            "total_predictions": total_predictions,
            "overall_accuracy": successful_predictions / total_predictions,
            "mean_confidence": np.mean([p["confidence"] for p in valid_predictions]),
            "severity_accuracy_breakdown": severity_accuracy
        }
    
    def calculate_per_type_metrics(self, per_image_analysis: Dict) -> Dict:
        """è®¡ç®—æ¯ç§è‰²ç›²ç±»å‹çš„æŒ‡æ ‡"""
        
        type_metrics = {}
        
        for cb_type in self.colorblind_types:
            type_predictions = []
            failure_thresholds = []
            
            for image_results in per_image_analysis.values():
                if cb_type in image_results["colorblind_results"]:
                    cb_results = image_results["colorblind_results"][cb_type]
                    type_predictions.extend([p for p in cb_results["predictions"] if "success" in p])
                    
                    if cb_results["failure_threshold"] is not None:
                        failure_thresholds.append(cb_results["failure_threshold"])
            
            if type_predictions:
                successful = sum(1 for p in type_predictions if p["success"])
                
                type_metrics[cb_type] = {
                    "total_predictions": len(type_predictions),
                    "accuracy": successful / len(type_predictions),
                    "mean_confidence": np.mean([p["confidence"] for p in type_predictions]),
                    "mean_failure_threshold": np.mean(failure_thresholds) if failure_thresholds else None,
                    "failure_threshold_std": np.std(failure_thresholds) if failure_thresholds else None
                }
        
        return type_metrics
    
    def analyze_boundaries(self, per_image_analysis: Dict) -> Dict:
        """åˆ†æè§†è§‰è¾¹ç•Œç‰¹å¾"""
        
        boundary_analysis = {
            "global_failure_distribution": {},
            "type_comparison": {},
            "difficulty_ranking": []
        }
        
        # æ”¶é›†æ‰€æœ‰å¤±è´¥é˜ˆå€¼
        all_failure_thresholds = []
        type_failure_thresholds = {cb_type: [] for cb_type in self.colorblind_types}
        
        for image_results in per_image_analysis.values():
            for cb_type, cb_results in image_results["colorblind_results"].items():
                if cb_results["failure_threshold"] is not None:
                    all_failure_thresholds.append(cb_results["failure_threshold"])
                    type_failure_thresholds[cb_type].append(cb_results["failure_threshold"])
        
        # å…¨å±€å¤±è´¥åˆ†å¸ƒ
        if all_failure_thresholds:
            boundary_analysis["global_failure_distribution"] = {
                "mean": np.mean(all_failure_thresholds),
                "std": np.std(all_failure_thresholds),
                "median": np.median(all_failure_thresholds),
                "min": np.min(all_failure_thresholds),
                "max": np.max(all_failure_thresholds)
            }
        
        # ç±»å‹é—´æ¯”è¾ƒ
        for cb_type, thresholds in type_failure_thresholds.items():
            if thresholds:
                boundary_analysis["type_comparison"][cb_type] = {
                    "mean_failure_threshold": np.mean(thresholds),
                    "robustness_score": 1.0 - np.mean(thresholds),  # å¤±è´¥é˜ˆå€¼è¶Šä½ï¼Œé²æ£’æ€§è¶Šå·®
                    "consistency": 1.0 / (1.0 + np.std(thresholds))  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šå¥½
                }
        
        return boundary_analysis
    
    def generate_evaluation_report(self, results: Dict, output_path: str = "evaluation_report.html"):
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>è‰²ç›²æµ‹è¯•æ•°æ®é›†æ¨¡å‹è¯„æµ‹æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .metric {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; }}
        .chart {{ margin: 20px 0; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <h1>è‰²ç›²æµ‹è¯•æ•°æ®é›†æ¨¡å‹è¯„æµ‹æŠ¥å‘Š</h1>
    
    <h2>æ€»ä½“æŒ‡æ ‡</h2>
    <div class="metric">
        <p><strong>æ€»é¢„æµ‹æ•°:</strong> {results['overall_metrics'].get('total_predictions', 'N/A')}</p>
        <p><strong>æ€»ä½“å‡†ç¡®ç‡:</strong> {results['overall_metrics'].get('overall_accuracy', 0):.3f}</p>
        <p><strong>å¹³å‡ç½®ä¿¡åº¦:</strong> {results['overall_metrics'].get('mean_confidence', 0):.3f}</p>
    </div>
    
    <h2>å„è‰²ç›²ç±»å‹è¡¨ç°</h2>
    <table>
        <tr>
            <th>è‰²ç›²ç±»å‹</th>
            <th>å‡†ç¡®ç‡</th>
            <th>å¹³å‡ç½®ä¿¡åº¦</th>
            <th>å¹³å‡å¤±è´¥é˜ˆå€¼</th>
            <th>é²æ£’æ€§è¯„åˆ†</th>
        </tr>
"""
        
        for cb_type, metrics in results["per_colorblind_type"].items():
            robustness = results["boundary_analysis"]["type_comparison"].get(cb_type, {}).get("robustness_score", 0)
            html_content += f"""
        <tr>
            <td>{cb_type}</td>
            <td>{metrics.get('accuracy', 0):.3f}</td>
            <td>{metrics.get('mean_confidence', 0):.3f}</td>
            <td>{metrics.get('mean_failure_threshold', 'N/A')}</td>
            <td>{robustness:.3f}</td>
        </tr>
"""
        
        html_content += """
    </table>
    
    <h2>è¾¹ç•Œåˆ†æ</h2>
    <div class="metric">
"""
        
        if "global_failure_distribution" in results["boundary_analysis"]:
            dist = results["boundary_analysis"]["global_failure_distribution"]
            html_content += f"""
        <p><strong>å¹³å‡å¤±è´¥é˜ˆå€¼:</strong> {dist.get('mean', 0):.3f}</p>
        <p><strong>å¤±è´¥é˜ˆå€¼æ ‡å‡†å·®:</strong> {dist.get('std', 0):.3f}</p>
        <p><strong>å¤±è´¥é˜ˆå€¼ä¸­ä½æ•°:</strong> {dist.get('median', 0):.3f}</p>
"""
        
        html_content += """
    </div>
    
    <h2>è¯„æµ‹è¯´æ˜</h2>
    <div class="metric">
        <p><strong>å¤±è´¥é˜ˆå€¼:</strong> æ¨¡å‹å¼€å§‹æ— æ³•æ­£ç¡®è¯†åˆ«çš„è‰²ç›²ä¸¥é‡ç¨‹åº¦</p>
        <p><strong>é²æ£’æ€§è¯„åˆ†:</strong> 1 - å¹³å‡å¤±è´¥é˜ˆå€¼ï¼Œè¶Šé«˜è¡¨ç¤ºæ¨¡å‹è¶Šé²æ£’</p>
        <p><strong>è‰²ç›²ä¸¥é‡ç¨‹åº¦:</strong> 0.0 = æ­£å¸¸è§†åŠ›ï¼Œ1.0 = å®Œå…¨è‰²ç›²</p>
    </div>
    
</body>
</html>
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"âœ“ è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def plot_accuracy_curves(self, results: Dict, output_dir: str = "evaluation_plots"):
        """ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿"""
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¸ºæ¯ç§è‰²ç›²ç±»å‹ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
        for cb_type in self.colorblind_types:
            plt.figure(figsize=(10, 6))
            
            all_severities = []
            all_accuracies = []
            
            for image_results in results["per_image_analysis"].values():
                if cb_type in image_results["colorblind_results"]:
                    cb_results = image_results["colorblind_results"][cb_type]
                    severities = [p["severity"] for p in cb_results["predictions"] if "severity" in p]
                    accuracies = cb_results["accuracy_curve"]
                    
                    if len(severities) == len(accuracies):
                        all_severities.extend(severities)
                        all_accuracies.extend(accuracies)
            
            if all_severities and all_accuracies:
                # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
                sorted_data = sorted(zip(all_severities, all_accuracies))
                severities, accuracies = zip(*sorted_data)
                
                plt.plot(severities, accuracies, 'o-', alpha=0.6, label=f'{cb_type}')
                plt.xlabel('è‰²ç›²ä¸¥é‡ç¨‹åº¦')
                plt.ylabel('å‡†ç¡®ç‡')
                plt.title(f'{cb_type.title()} å‡†ç¡®ç‡ vs è‰²ç›²ä¸¥é‡ç¨‹åº¦')
                plt.grid(True, alpha=0.3)
                plt.legend()
                
                plt.tight_layout()
                plt.savefig(output_path / f'{cb_type}_accuracy_curve.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        print(f"âœ“ å‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜åˆ°: {output_path}")
    
    @staticmethod
    def is_prediction_correct(prediction: str, expected: str) -> bool:
        """åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´æ¯”è¾ƒé€»è¾‘
        if expected.lower() == "unknown":
            return True  # å¦‚æœæœŸæœ›ç­”æ¡ˆæœªçŸ¥ï¼Œè®¤ä¸ºé¢„æµ‹æ­£ç¡®
        
        # æå–æ•°å­—æˆ–å…³é”®è¯è¿›è¡Œæ¯”è¾ƒ
        import re
        pred_numbers = re.findall(r'\d+', str(prediction))
        expected_numbers = re.findall(r'\d+', str(expected))
        
        if pred_numbers and expected_numbers:
            return pred_numbers[0] == expected_numbers[0]
        
        # å…³é”®è¯æ¯”è¾ƒ
        return str(prediction).lower() == str(expected).lower()
    
    def extract_expected_answer(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æœŸæœ›ç­”æ¡ˆ"""
        import re
        filename_lower = filename.lower()
        
        # æ£€æŸ¥æ•°å­—
        numbers = re.findall(r'\d+', filename)
        if numbers:
            return numbers[0]
        
        # æ£€æŸ¥å…³é”®è¯
        keywords = ['circle', 'square', 'triangle', 'diamond', 'star']
        for keyword in keywords:
            if keyword in filename_lower:
                return keyword
        
        return "unknown"


# ç¤ºä¾‹ä½¿ç”¨
def example_model_predict(image_path: str) -> Tuple[str, float]:
    """
    ç¤ºä¾‹æ¨¡å‹é¢„æµ‹å‡½æ•°
    å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹
    
    Args:
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        
    Returns:
        (é¢„æµ‹ç»“æœ, ç½®ä¿¡åº¦)
    """
    # è¿™é‡Œæ˜¯ç¤ºä¾‹å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹
    import random
    
    # æ¨¡æ‹Ÿé¢„æµ‹é€»è¾‘ï¼šéšæœºè¿”å›æ•°å­—å’Œç½®ä¿¡åº¦
    predicted_number = str(random.randint(1, 9))
    confidence = random.uniform(0.3, 0.95)
    
    return predicted_number, confidence


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¯„æµ‹æ¡†æ¶"""
    
    print("ğŸ¯ è‰²ç›²æµ‹è¯•æ•°æ®é›†æ¨¡å‹è¯„æµ‹æ¡†æ¶")
    print("=" * 50)
    
    # åˆå§‹åŒ–è¯„æµ‹å™¨
    evaluator = ColorBlindnessEvaluator()
    
    # è¿è¡Œè¯„æµ‹ï¼ˆä½¿ç”¨ç¤ºä¾‹æ¨¡å‹ï¼‰
    print("ä½¿ç”¨ç¤ºä¾‹æ¨¡å‹è¿›è¡Œè¯„æµ‹...")
    results = evaluator.evaluate_model_boundary(
        model_predict_func=example_model_predict,
        confidence_threshold=0.7
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_evaluation_report(results)
    evaluator.plot_accuracy_curves(results)
    
    # ä¿å­˜ç»“æœ
    with open("evaluation_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\nâœ… è¯„æµ‹å®Œæˆï¼")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("  - evaluation_report.html: è¯¦ç»†è¯„æµ‹æŠ¥å‘Š")
    print("  - evaluation_plots/: å‡†ç¡®ç‡æ›²çº¿å›¾")
    print("  - evaluation_results.json: å®Œæ•´è¯„æµ‹æ•°æ®")


if __name__ == "__main__":
    main()