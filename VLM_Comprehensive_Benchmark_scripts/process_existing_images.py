#!/usr/bin/env python3
"""
å¤„ç†ç°æœ‰å›¾åƒ - ç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„å›¾åƒç”Ÿæˆè‰²ç›²æµ‹è¯•æ•°æ®é›†
"""

import sys
from pathlib import Path

# æ·»åŠ scriptsç›®å½•åˆ°Pythonè·¯å¾„
scripts_dir = Path(__file__).parent / "scripts"
sys.path.insert(0, str(scripts_dir))

from scripts.colorblind_simulation import ColorBlindnessSimulator
import json
import time
from PIL import Image

def main():
    print("ğŸ¨ è‰²ç›²æµ‹è¯•æ•°æ®é›†å¤„ç†å™¨")
    print("=" * 50)
    print("ä½¿ç”¨ç°æœ‰çš„çœŸå®ç½‘ç»œå›¾åƒç”Ÿæˆå®Œæ•´æ•°æ®é›†")
    
    # æ£€æŸ¥ç°æœ‰å›¾åƒ
    raw_dir = Path("data/raw")
    if not raw_dir.exists():
        print("âŒ æœªæ‰¾åˆ°data/rawç›®å½•")
        return False
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_extensions = {'.png', '.jpg', '.jpeg', '.bmp'}
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(raw_dir.glob(f"*{ext}"))
    
    if not image_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        return False
    
    print(f"âœ“ æ‰¾åˆ° {len(image_files)} å¼ çœŸå®ç½‘ç»œå›¾åƒ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    gradients_dir = Path("data/gradients")
    gradients_dir.mkdir(parents=True, exist_ok=True)
    
    metadata_dir = Path("metadata")
    metadata_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–è‰²ç›²æ¨¡æ‹Ÿå™¨
    simulator = ColorBlindnessSimulator()
    colorblind_types = ['protanopia', 'deuteranopia', 'tritanopia']
    gradient_steps = 100
    
    print(f"\nå¼€å§‹å¤„ç†å›¾åƒ...")
    print(f"- è‰²ç›²ç±»å‹: {len(colorblind_types)} ç§")
    print(f"- æ¢¯åº¦æ­¥æ•°: {gradient_steps} æ­¥")
    print(f"- é¢„è®¡ç”Ÿæˆ: {len(image_files) * len(colorblind_types) * (gradient_steps + 1)} å¼ å›¾åƒ")
    
    dataset_metadata = {
        "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_base_images": len(image_files),
        "colorblind_types": colorblind_types,
        "gradient_steps": gradient_steps,
        "images": []
    }
    
    total_generated = 0
    
    for i, image_file in enumerate(image_files):
        print(f"\nå¤„ç†å›¾åƒ {i+1}/{len(image_files)}: {image_file.name}")
        
        try:
            # åŠ è½½å¹¶éªŒè¯å›¾åƒ
            image = Image.open(image_file).convert('RGB')
            print(f"  å›¾åƒå°ºå¯¸: {image.size}")
            
            # ä¸ºæ¯ç§è‰²ç›²ç±»å‹ç”Ÿæˆæ¢¯åº¦
            image_metadata = {
                "base_image": image_file.name,
                "base_image_path": str(image_file),
                "image_size": image.size,
                "colorblind_variants": {}
            }
            
            for colorblind_type in colorblind_types:
                print(f"  ç”Ÿæˆ {colorblind_type} æ¢¯åº¦...")
                
                # åˆ›å»ºè¾“å‡ºç›®å½•
                type_output_dir = gradients_dir / image_file.stem / colorblind_type
                type_output_dir.mkdir(parents=True, exist_ok=True)
                
                # ç”Ÿæˆæ¢¯åº¦åºåˆ—
                generated_files = []
                
                for step in range(gradient_steps + 1):
                    severity = step / gradient_steps
                    
                    # åº”ç”¨è‰²ç›²æ¨¡æ‹Ÿ
                    sim_func = getattr(simulator, f'simulate_{colorblind_type}')
                    simulated_image = sim_func(image, severity)
                    
                    # ä¿å­˜å›¾åƒ
                    filename = f"step_{step:03d}_severity_{severity:.2f}.png"
                    filepath = type_output_dir / filename
                    simulated_image.save(filepath)
                    generated_files.append(str(filepath))
                    
                    if step % 20 == 0:  # æ¯20æ­¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        print(f"    è¿›åº¦: {step}/{gradient_steps}")
                
                # åˆ†æå¯¹æ¯”åº¦å˜åŒ–
                try:
                    contrast_analysis = simulator.analyze_color_contrast(
                        image, colorblind_type, 1.0
                    )
                except Exception as e:
                    print(f"    è­¦å‘Š: å¯¹æ¯”åº¦åˆ†æå¤±è´¥: {e}")
                    contrast_analysis = {"error": str(e)}
                
                variant_metadata = {
                    "colorblind_type": colorblind_type,
                    "generated_files": generated_files,
                    "num_gradients": len(generated_files),
                    "contrast_analysis": contrast_analysis,
                    "output_directory": str(type_output_dir)
                }
                
                image_metadata["colorblind_variants"][colorblind_type] = variant_metadata
                total_generated += len(generated_files)
                
                print(f"    âœ“ ç”Ÿæˆäº† {len(generated_files)} ä¸ªæ¢¯åº¦æ–‡ä»¶")
            
            dataset_metadata["images"].append(image_metadata)
            
        except Exception as e:
            print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
            continue
    
    # ä¿å­˜æ•°æ®é›†å…ƒæ•°æ®
    metadata_file = metadata_dir / "final_dataset.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_metadata, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        "dataset_overview": {
            "total_base_images": len(image_files),
            "total_gradient_images": total_generated,
            "colorblind_types": colorblind_types,
            "gradient_steps": gradient_steps,
            "processing_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        },
        "images_per_type": {},
        "file_distribution": {}
    }
    
    # ç»Ÿè®¡æ¯ç§è‰²ç›²ç±»å‹çš„å›¾åƒæ•°é‡
    for cb_type in colorblind_types:
        count = 0
        for img_meta in dataset_metadata["images"]:
            if cb_type in img_meta["colorblind_variants"]:
                count += img_meta["colorblind_variants"][cb_type]["num_gradients"]
        stats["images_per_type"][cb_type] = count
    
    stats_file = metadata_dir / "final_statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
    test_cases = []
    for img_meta in dataset_metadata["images"]:
        base_image = img_meta["base_image"]
        expected_answer = extract_expected_answer(base_image)
        
        for cb_type, variant_meta in img_meta["colorblind_variants"].items():
            test_sequence = {
                "test_id": f"{Path(base_image).stem}_{cb_type}",
                "base_image": base_image,
                "colorblind_type": cb_type,
                "expected_answer": expected_answer,
                "test_description": f"æµ‹è¯•æ¨¡å‹åœ¨{cb_type}æ¨¡æ‹Ÿä¸‹è¯†åˆ«{expected_answer}çš„èƒ½åŠ›",
                "gradient_files": variant_meta["generated_files"],
                "num_gradients": variant_meta["num_gradients"]
            }
            test_cases.append(test_sequence)
    
    test_cases_file = metadata_dir / "test_cases.json"
    with open(test_cases_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total_test_sequences": len(test_cases),
            "creation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_sequences": test_cases
        }, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆREADME
    generate_readme(len(image_files), total_generated, colorblind_types, gradient_steps)
    
    print(f"\n" + "=" * 50)
    print("ğŸ‰ æ•°æ®é›†å¤„ç†å®Œæˆ!")
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"- åŸºç¡€å›¾åƒ: {len(image_files)} å¼ çœŸå®ç½‘ç»œå›¾åƒ")
    print(f"- ç”Ÿæˆå›¾åƒ: {total_generated} å¼ ")
    print(f"- è‰²ç›²ç±»å‹: {', '.join(colorblind_types)}")
    print(f"- æµ‹è¯•åºåˆ—: {len(test_cases)} ä¸ª")
    
    print(f"\nğŸ“ æ–‡ä»¶ä½ç½®:")
    print(f"- æ¢¯åº¦å›¾åƒ: data/gradients/")
    print(f"- å…ƒæ•°æ®: {metadata_file}")
    print(f"- ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
    print(f"- æµ‹è¯•ç”¨ä¾‹: {test_cases_file}")
    
    return True

def extract_expected_answer(filename):
    """ä»æ–‡ä»¶åæå–æœŸæœ›ç­”æ¡ˆ"""
    filename_lower = filename.lower()
    
    # æ£€æŸ¥æ•°å­—
    import re
    numbers = re.findall(r'\d+', filename)
    if numbers:
        return numbers[0]
    
    # æ£€æŸ¥å…³é”®è¯
    if 'circle' in filename_lower:
        return 'circle'
    elif 'square' in filename_lower:
        return 'square'
    elif 'triangle' in filename_lower:
        return 'triangle'
    
    return 'unknown'

def generate_readme(base_count, total_count, colorblind_types, gradient_steps):
    """ç”ŸæˆREADMEæ–‡æ¡£"""
    readme_content = f"""# è‰²ç›²æµ‹è¯•æ•°æ®é›†

## æ¦‚è¿°
åŸºäºçœŸå®ç½‘ç»œçŸ³åŸæ°è‰²ç›²æµ‹è¯•å›¾çš„AIè§†è§‰è¾¹ç•Œè¯„ä¼°æ•°æ®é›†ã€‚

## æ•°æ®é›†ç»Ÿè®¡
- **åŸºç¡€å›¾åƒ**: {base_count} å¼ çœŸå®ç½‘ç»œå›¾åƒ
- **ç”Ÿæˆå›¾åƒ**: {total_count} å¼ æ¢¯åº¦å˜åŒ–å›¾åƒ
- **è‰²ç›²ç±»å‹**: {len(colorblind_types)} ç§ ({', '.join(colorblind_types)})
- **æ¢¯åº¦æ­¥æ•°**: {gradient_steps} æ­¥ (0%-100%ä¸¥é‡ç¨‹åº¦)

## æ•°æ®æ¥æº
æ‰€æœ‰åŸºç¡€å›¾åƒå‡æ¥è‡ªåˆæ³•çš„ç½‘ç»œå¼€æºèµ„æºï¼š
- GitHubå¼€æºä»“åº“ (ä¸»è¦æ¥æº)
- ç»´åŸºåª’ä½“å…±äº«èµ„æº
- åŒ»å­¦æ•™è‚²ç½‘ç«™

## æ•°æ®é›†ç»“æ„
```
data/
â”œâ”€â”€ raw/                    # åŸå§‹çœŸå®å›¾åƒ ({base_count}å¼ )
â””â”€â”€ gradients/             # è‰²ç›²æ¨¡æ‹Ÿæ¢¯åº¦å›¾åƒ
    â””â”€â”€ [image_name]/
        â”œâ”€â”€ protanopia/    # çº¢è‰²ç›²æ¨¡æ‹Ÿ
        â”œâ”€â”€ deuteranopia/  # ç»¿è‰²ç›²æ¨¡æ‹Ÿ
        â””â”€â”€ tritanopia/    # è“è‰²ç›²æ¨¡æ‹Ÿ
```

## æµ‹è¯•ç›®æ ‡
è¯„ä¼°AIæ¨¡å‹åœ¨ä¸åŒè‰²ç›²æ¨¡æ‹Ÿç¨‹åº¦ä¸‹è¯†åˆ«éšè—æ•°å­—/ç¬¦å·çš„èƒ½åŠ›è¾¹ç•Œã€‚

ç”Ÿæˆæ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S")}
"""
    
    docs_dir = Path("docs")
    docs_dir.mkdir(exist_ok=True)
    
    readme_file = docs_dir / "README.md"
    with open(readme_file, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ“ READMEæ–‡æ¡£ä¿å­˜åˆ°: {readme_file}")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)