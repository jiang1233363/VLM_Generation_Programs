#!/usr/bin/env python3
"""
è§†è§‰è¾¹ç•Œæµ‹è¯•è¯„ä¼°æ¡†æ¶
ç”¨äºæµ‹è¯•AIæ¨¡å‹åœ¨ä¸åŒè§†è§‰é€€åŒ–çº§åˆ«ä¸‹çš„æ€§èƒ½è¾¹ç•Œ
"""

import json
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Callable
import matplotlib.pyplot as plt
import pandas as pd
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    image_name: str
    degradation_type: str
    level: int
    success: bool
    confidence: float
    processing_time: float
    error_message: Optional[str] = None

class VisualBoundaryEvaluator:
    """è§†è§‰è¾¹ç•Œè¯„ä¼°å™¨"""
    
    def __init__(self, base_dir: str = "/home/jgy/visual_boundary_dataset"):
        self.base_dir = Path(base_dir)
        self.degraded_dir = self.base_dir / "degraded_images"
        self.results_dir = self.base_dir / "evaluation_results"
        self.metadata_dir = self.base_dir / "metadata"
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir.mkdir(exist_ok=True)
        
        # æ”¯æŒçš„é€€åŒ–ç±»å‹
        self.degradation_types = [
            'sharpness', 'brightness', 'contrast', 
            'color_distortion', 'color_shift', 'resolution'
        ]
        
        # åŠ è½½åŸºç¡€å›¾ç‰‡ä¿¡æ¯
        self.base_images = self.load_base_images()
        
    def load_base_images(self) -> List[Dict]:
        """åŠ è½½åŸºç¡€å›¾ç‰‡ä¿¡æ¯"""
        try:
            metadata_file = self.metadata_dir / "final_image_collection.json"
            with open(metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data['selected_images']
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½åŸºç¡€å›¾ç‰‡ä¿¡æ¯: {e}")
            return []
    
    def get_degraded_image_path(self, base_filename: str, degradation_type: str, level: int) -> Path:
        """è·å–é€€åŒ–å›¾ç‰‡è·¯å¾„"""
        # ä»selected_filenameè·å–åŸºç¡€åç§°
        if 'selected_filename' in base_filename:
            base_name = Path(base_filename['selected_filename']).stem
        else:
            base_name = Path(base_filename).stem
            
        filename = f"{base_name}_{degradation_type}_level_{level:03d}.jpg"
        return self.degraded_dir / degradation_type / filename
    
    def dummy_ai_model(self, image_path: Path) -> Dict:
        """
        æ¨¡æ‹ŸAIæ¨¡å‹æµ‹è¯•å‡½æ•°
        å®é™…ä½¿ç”¨æ—¶ï¼Œæ›¿æ¢ä¸ºçœŸå®çš„AIæ¨¡å‹è°ƒç”¨
        """
        try:
            # åŠ è½½å›¾ç‰‡
            image = cv2.imread(str(image_path))
            if image is None:
                return {
                    'success': False,
                    'confidence': 0.0,
                    'processing_time': 0.0,
                    'error': 'Cannot load image'
                }
            
            # æ¨¡æ‹Ÿå›¾ç‰‡è´¨é‡è¯„ä¼°
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # è®¡ç®—å›¾ç‰‡è´¨é‡æŒ‡æ ‡
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            brightness = np.mean(gray)
            contrast = gray.std()
            
            # æ¨¡æ‹ŸAIæ¨¡å‹çš„æ€§èƒ½è¡°å‡
            # åŸºäºå›¾ç‰‡è´¨é‡è®¡ç®—æˆåŠŸæ¦‚ç‡å’Œç½®ä¿¡åº¦
            quality_score = (
                min(sharpness / 500, 1.0) * 0.4 +
                (1.0 - abs(brightness - 127) / 127) * 0.3 +
                min(contrast / 80, 1.0) * 0.3
            )
            
            # æ·»åŠ éšæœºå™ªå£°æ¨¡æ‹ŸçœŸå®æ¨¡å‹çš„ä¸ç¡®å®šæ€§
            noise = np.random.normal(0, 0.1)
            final_score = np.clip(quality_score + noise, 0, 1)
            
            # è®¾å®šæˆåŠŸé˜ˆå€¼
            success_threshold = 0.3
            success = final_score > success_threshold
            confidence = final_score if success else (1 - final_score)
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            processing_time = np.random.uniform(0.1, 0.5)
            
            return {
                'success': bool(success),
                'confidence': float(confidence),
                'processing_time': processing_time,
                'error': None
            }
            
        except Exception as e:
            return {
                'success': False,
                'confidence': 0.0,
                'processing_time': 0.0,
                'error': str(e)
            }
    
    def test_single_image(self, 
                         base_image: Dict, 
                         degradation_type: str,
                         test_levels: List[int],
                         model_function: Callable) -> List[TestResult]:
        """æµ‹è¯•å•å¼ å›¾ç‰‡åœ¨æŒ‡å®šé€€åŒ–ç±»å‹ä¸‹çš„æ€§èƒ½"""
        results = []
        
        base_filename = base_image.get('selected_filename', base_image['filename'])
        image_name = Path(base_filename).stem
        
        print(f"ğŸ“¸ æµ‹è¯•å›¾ç‰‡: {image_name} - {degradation_type}")
        
        for level in test_levels:
            image_path = self.get_degraded_image_path(base_image, degradation_type, level)
            
            if not image_path.exists():
                print(f"  âŒ Level {level}: å›¾ç‰‡ä¸å­˜åœ¨")
                results.append(TestResult(
                    image_name=image_name,
                    degradation_type=degradation_type,
                    level=level,
                    success=False,
                    confidence=0.0,
                    processing_time=0.0,
                    error_message="Image file not found"
                ))
                continue
            
            # è°ƒç”¨AIæ¨¡å‹æµ‹è¯•å‡½æ•°
            model_result = model_function(image_path)
            
            result = TestResult(
                image_name=image_name,
                degradation_type=degradation_type,
                level=level,
                success=model_result['success'],
                confidence=model_result['confidence'],
                processing_time=model_result['processing_time'],
                error_message=model_result.get('error')
            )
            
            results.append(result)
            
            # æ˜¾ç¤ºè¿›åº¦
            status = "âœ“" if result.success else "âŒ"
            print(f"  {status} Level {level}: æˆåŠŸ={result.success}, ç½®ä¿¡åº¦={result.confidence:.3f}")
        
        return results
    
    def find_failure_threshold(self, results: List[TestResult]) -> Dict:
        """æ‰¾åˆ°æ¨¡å‹å¤±è´¥çš„é˜ˆå€¼çº§åˆ«"""
        # æŒ‰çº§åˆ«æ’åº
        sorted_results = sorted(results, key=lambda x: x.level)
        
        # æ‰¾åˆ°è¿ç»­å¤±è´¥çš„èµ·å§‹ç‚¹
        failure_threshold = None
        success_count = 0
        failure_count = 0
        
        for result in sorted_results:
            if result.success:
                success_count += 1
                failure_count = 0
            else:
                failure_count += 1
                success_count = 0
                
                # å¦‚æœè¿ç»­å¤±è´¥3æ¬¡ï¼Œè®¤ä¸ºè¾¾åˆ°å¤±è´¥é˜ˆå€¼
                if failure_count >= 3 and failure_threshold is None:
                    failure_threshold = result.level - 2
                    break
        
        # è®¡ç®—æˆåŠŸç‡
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.success)
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = np.mean([r.confidence for r in results])
        
        return {
            'failure_threshold': failure_threshold,
            'success_rate': success_rate,
            'avg_confidence': avg_confidence,
            'total_tests': total_tests,
            'successful_tests': successful_tests
        }
    
    def run_comprehensive_evaluation(self, 
                                   model_function: Optional[Callable] = None,
                                   test_images: Optional[List[str]] = None,
                                   test_levels: Optional[List[int]] = None) -> Dict:
        """è¿è¡Œç»¼åˆè¯„ä¼°"""
        
        print("ğŸ¯ å¼€å§‹è§†è§‰è¾¹ç•Œç»¼åˆè¯„ä¼°")
        print("=" * 60)
        
        # ä½¿ç”¨é»˜è®¤å‚æ•°
        if model_function is None:
            model_function = self.dummy_ai_model
            print("ğŸ“ ä½¿ç”¨æ¨¡æ‹ŸAIæ¨¡å‹è¿›è¡Œæµ‹è¯•")
        
        if test_levels is None:
            # æµ‹è¯•å…³é”®çº§åˆ«: 0, 10, 20, ..., 100
            test_levels = list(range(0, 101, 10))
            
        # é€‰æ‹©æµ‹è¯•å›¾ç‰‡
        if test_images is None:
            # é€‰æ‹©å‰10å¼ é«˜è´¨é‡å›¾ç‰‡è¿›è¡Œæµ‹è¯•
            test_images = self.base_images[:10]
        else:
            # æ ¹æ®æ–‡ä»¶åç­›é€‰
            test_images = [img for img in self.base_images 
                          if any(name in img['filename'] for name in test_images)]
        
        print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
        print(f"   æµ‹è¯•å›¾ç‰‡: {len(test_images)} å¼ ")
        print(f"   é€€åŒ–ç±»å‹: {len(self.degradation_types)} ç§")
        print(f"   æµ‹è¯•çº§åˆ«: {len(test_levels)} ä¸ª {test_levels}")
        print(f"   é¢„è®¡æ€»æµ‹è¯•: {len(test_images) * len(self.degradation_types) * len(test_levels)} æ¬¡")
        
        # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
        all_results = []
        summary_stats = {}
        
        # é€ç±»å‹æµ‹è¯•
        for degradation_type in self.degradation_types:
            print(f"\\nğŸ”¬ æµ‹è¯•é€€åŒ–ç±»å‹: {degradation_type.upper()}")
            print("-" * 40)
            
            type_results = []
            
            for base_image in test_images:
                image_results = self.test_single_image(
                    base_image, degradation_type, test_levels, model_function
                )
                type_results.extend(image_results)
                all_results.extend(image_results)
            
            # åˆ†æè¯¥ç±»å‹çš„ç»“æœ
            threshold_analysis = self.find_failure_threshold(type_results)
            summary_stats[degradation_type] = threshold_analysis
            
            print(f"ğŸ“ˆ {degradation_type} ç»“æœæ€»ç»“:")
            print(f"   å¤±è´¥é˜ˆå€¼: Level {threshold_analysis['failure_threshold']}")
            print(f"   æˆåŠŸç‡: {threshold_analysis['success_rate']:.1%}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {threshold_analysis['avg_confidence']:.3f}")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        evaluation_results = {
            'test_configuration': {
                'test_images_count': len(test_images),
                'degradation_types': self.degradation_types,
                'test_levels': test_levels,
                'total_tests': len(all_results)
            },
            'detailed_results': [
                {
                    'image_name': r.image_name,
                    'degradation_type': r.degradation_type,
                    'level': r.level,
                    'success': r.success,
                    'confidence': r.confidence,
                    'processing_time': r.processing_time,
                    'error_message': r.error_message
                }
                for r in all_results
            ],
            'summary_statistics': summary_stats,
            'overall_metrics': {
                'total_tests': len(all_results),
                'successful_tests': sum(1 for r in all_results if r.success),
                'overall_success_rate': sum(1 for r in all_results if r.success) / len(all_results),
                'avg_processing_time': np.mean([r.processing_time for r in all_results]),
                'avg_confidence': np.mean([r.confidence for r in all_results])
            }
        }
        
        # ä¿å­˜ç»“æœ
        results_file = self.results_dir / f"evaluation_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        
        print(f"\\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“„ ç»“æœä¿å­˜: {results_file}")
        print(f"\\nğŸ“Š æ€»ä½“æ€§èƒ½:")
        print(f"   æ€»æµ‹è¯•æ¬¡æ•°: {evaluation_results['overall_metrics']['total_tests']}")
        print(f"   æ€»ä½“æˆåŠŸç‡: {evaluation_results['overall_metrics']['overall_success_rate']:.1%}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {evaluation_results['overall_metrics']['avg_confidence']:.3f}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {evaluation_results['overall_metrics']['avg_processing_time']:.3f}ç§’")
        
        return evaluation_results
    
    def generate_visualization(self, results: Dict):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("\\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        charts_dir = self.results_dir / "charts"
        charts_dir.mkdir(exist_ok=True)
        
        # 1. å„é€€åŒ–ç±»å‹çš„å¤±è´¥é˜ˆå€¼å¯¹æ¯”
        degradation_types = list(results['summary_statistics'].keys())
        failure_thresholds = [
            results['summary_statistics'][dt]['failure_threshold'] or 100 
            for dt in degradation_types
        ]
        
        plt.figure(figsize=(12, 6))
        bars = plt.bar(degradation_types, failure_thresholds, 
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])
        plt.title('AIæ¨¡å‹åœ¨ä¸åŒé€€åŒ–ç±»å‹ä¸‹çš„å¤±è´¥é˜ˆå€¼', fontsize=14, fontweight='bold')
        plt.xlabel('é€€åŒ–ç±»å‹')
        plt.ylabel('å¤±è´¥é˜ˆå€¼çº§åˆ«')
        plt.ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, failure_thresholds):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{int(value)}', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(charts_dir / 'failure_thresholds.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æˆåŠŸç‡å¯¹æ¯”
        success_rates = [
            results['summary_statistics'][dt]['success_rate'] * 100
            for dt in degradation_types
        ]
        
        plt.figure(figsize=(12, 6))
        bars = plt.bar(degradation_types, success_rates,
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])
        plt.title('AIæ¨¡å‹åœ¨ä¸åŒé€€åŒ–ç±»å‹ä¸‹çš„æˆåŠŸç‡', fontsize=14, fontweight='bold')
        plt.xlabel('é€€åŒ–ç±»å‹')
        plt.ylabel('æˆåŠŸç‡ (%)')
        plt.ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, success_rates):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(charts_dir / 'success_rates.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š å›¾è¡¨ä¿å­˜åˆ°: {charts_dir}")
        
        return charts_dir

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè¯„ä¼°æ¡†æ¶çš„ä½¿ç”¨"""
    evaluator = VisualBoundaryEvaluator()
    
    # è¿è¡Œç»¼åˆè¯„ä¼°
    results = evaluator.run_comprehensive_evaluation()
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    evaluator.generate_visualization(results)
    
    print("\\nğŸ‰ è§†è§‰è¾¹ç•Œè¯„ä¼°æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    main()