#!/usr/bin/env python3
"""
VLM Benchmark æ•°æ®é›†æ•´ç†è„šæœ¬
æŒ‰ç…§Subjectã€Relationã€Attributeã€Illusionå››å¤§ç±»åˆ«æ•´ç†ç°æœ‰æ•°æ®
"""

import os
import shutil
import json
from pathlib import Path
from datetime import datetime

class VLMBenchmarkOrganizer:
    def __init__(self):
        self.base_path = Path("/home/jgy")
        self.output_path = Path("/home/jgy/VLM_Comprehensive_Benchmark")
        
        # æ•°æ®é›†åˆ†ç±»æ˜ å°„
        self.dataset_mapping = {
            "Subject": {
                "description": "VLMå¯¹å›¾åƒä¸­å•ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹ä¸»ä½“çš„è¯†åˆ«ã€æ„ŸçŸ¥å’Œå±æ€§ç†è§£èƒ½åŠ›",
                "subcategories": {
                    "clarity_degradation": "æ¸…æ™°åº¦é€€åŒ–æ„ŸçŸ¥",
                    "brightness_variation": "äº®åº¦å˜åŒ–æ„ŸçŸ¥", 
                    "contrast_variation": "å¯¹æ¯”åº¦å˜åŒ–æ„ŸçŸ¥",
                    "color_distortion": "é¢œè‰²å¤±çœŸæ„ŸçŸ¥",
                    "color_shift": "è‰²åè¯†åˆ«",
                    "fine_grained_classification": "ç»†ç²’åº¦ä¸»ä½“åˆ†ç±»",
                    "colorblind_recognition": "è‰²ç›²è¯†åˆ«",
                    "resolution_variation": "åˆ†è¾¨ç‡å˜åŒ–"
                },
                "source_datasets": [
                    "/home/jgy/Real_World_Noise_Dataset",
                    "/home/jgy/visual_boundary_dataset"
                ]
            },
            "Relation": {
                "description": "VLMå¯¹å›¾åƒä¸­å¤šä¸ªä¸»ä½“ä¹‹é—´ç©ºé—´ã€æ—¶é—´ã€å› æœå’Œé€»è¾‘å…³ç³»çš„ç†è§£èƒ½åŠ›",
                "subcategories": {
                    "spatial_relations": "ç©ºé—´ä½ç½®å…³ç³»",
                    "proximity_relations": "è·ç¦»/é è¿‘å…³ç³»",
                    "alignment_relations": "å¯¹é½/æ–¹å‘å…³ç³»", 
                    "comparative_relations": "æ¯”è¾ƒå…³ç³»"
                },
                "source_datasets": []
            },
            "Attribute": {
                "description": "VLMå¯¹å›¾åƒå±æ€§çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›",
                "subcategories": {
                    "global_noise": "å›¾åƒæ•´ä½“åŠ å™ªå£°",
                    "pixel_manipulation": "åƒç´ ç‚¹æ“ä½œ"
                },
                "source_datasets": [
                    "/home/jgy/Real_World_Noise_Dataset"
                ]
            },
            "Illusion": {
                "description": "VLMå¯¹è§†è§‰é”™è§‰çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›",
                "subcategories": {
                    "geometric_illusions": "å‡ ä½•é”™è§‰",
                    "color_illusions": "è‰²å½©é”™è§‰",
                    "motion_illusions": "è¿åŠ¨é”™è§‰",
                    "ambiguous_figures": "æ¨¡ç³Šå›¾å½¢"
                },
                "source_datasets": [
                    "/home/jgy/Unified_Illusion_Dataset"
                ]
            }
        }
        
        # åˆå§‹åŒ–ç›®å½•
        self.setup_directories()

    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        self.output_path.mkdir(exist_ok=True)
        
        # ä¸ºå››å¤§ç±»åˆ«åˆ›å»ºç›®å½•
        for category in self.dataset_mapping.keys():
            category_path = self.output_path / category
            category_path.mkdir(exist_ok=True)
            
            # ä¸ºå­ç±»åˆ«åˆ›å»ºç›®å½•
            subcategories = self.dataset_mapping[category]["subcategories"]
            for subcat_key, subcat_name in subcategories.items():
                subcat_path = category_path / subcat_key
                subcat_path.mkdir(exist_ok=True)

    def analyze_existing_datasets(self):
        """åˆ†æç°æœ‰æ•°æ®é›†"""
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "categories": {}
        }
        
        for category, info in self.dataset_mapping.items():
            category_analysis = {
                "description": info["description"],
                "subcategories": info["subcategories"],
                "datasets": []
            }
            
            for dataset_path in info["source_datasets"]:
                if Path(dataset_path).exists():
                    dataset_info = self.analyze_dataset(dataset_path)
                    category_analysis["datasets"].append(dataset_info)
            
            analysis["categories"][category] = category_analysis
        
        return analysis

    def analyze_dataset(self, dataset_path):
        """åˆ†æå•ä¸ªæ•°æ®é›†"""
        path = Path(dataset_path)
        
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        total_images = 0
        total_dirs = 0
        
        for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:
            total_images += len(list(path.glob(f"**/*{ext}")))
        
        total_dirs = len([d for d in path.glob("**/*") if d.is_dir()])
        
        return {
            "path": str(path),
            "name": path.name,
            "total_images": total_images,
            "total_directories": total_dirs,
            "size_mb": self.get_directory_size(path)
        }

    def get_directory_size(self, path):
        """è·å–ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        try:
            total = 0
            for dirpath, dirnames, filenames in os.walk(path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total += os.path.getsize(filepath)
            return round(total / (1024 * 1024), 2)
        except:
            return 0

    def organize_subject_data(self):
        """æ•´ç†Subjectç±»åˆ«æ•°æ®"""
        print("ğŸ¯ æ•´ç†Subjectç±»åˆ«æ•°æ®...")
        
        subject_path = self.output_path / "Subject"
        
        # 1. å¤„ç†Real_World_Noise_Dataset - å¯¹åº”å¤šä¸ªSubjectå­ç±»åˆ«
        noise_dataset = self.base_path / "Real_World_Noise_Dataset"
        if noise_dataset.exists():
            # å™ªå£°æ¢¯åº¦ -> æ¸…æ™°åº¦é€€åŒ–
            if (noise_dataset / "noise_gradients").exists():
                clarity_path = subject_path / "clarity_degradation"
                self.copy_with_symlink(noise_dataset / "noise_gradients", 
                                     clarity_path / "noise_gradients")
            
            # æ¨¡ç³Šæ¢¯åº¦ -> æ¸…æ™°åº¦é€€åŒ–  
            if (noise_dataset / "blur_gradients").exists():
                clarity_path = subject_path / "clarity_degradation"
                self.copy_with_symlink(noise_dataset / "blur_gradients",
                                     clarity_path / "blur_gradients")
            
            # äº®åº¦å˜åŒ–
            if (noise_dataset / "brightness_variation").exists():
                brightness_path = subject_path / "brightness_variation"
                self.copy_with_symlink(noise_dataset / "brightness_variation",
                                     brightness_path / "brightness_data")
            
            # å¯¹æ¯”åº¦å˜åŒ–
            if (noise_dataset / "contrast_variation").exists():
                contrast_path = subject_path / "contrast_variation" 
                self.copy_with_symlink(noise_dataset / "contrast_variation",
                                     contrast_path / "contrast_data")
            
            # è‰²å½©å˜åŒ–
            if (noise_dataset / "color_shift").exists():
                color_path = subject_path / "color_shift"
                self.copy_with_symlink(noise_dataset / "color_shift",
                                     color_path / "color_shift_data")

        # 2. å¤„ç†visual_boundary_dataset
        boundary_dataset = self.base_path / "visual_boundary_dataset"
        if boundary_dataset.exists():
            # åŸå§‹å¤šæ ·åŒ–å›¾ç‰‡ -> ç»†ç²’åº¦åˆ†ç±»
            if (boundary_dataset / "downloaded_images").exists():
                fine_grained_path = subject_path / "fine_grained_classification"
                self.copy_with_symlink(boundary_dataset / "downloaded_images",
                                     fine_grained_path / "diverse_images")
            
            # é€€åŒ–å›¾ç‰‡ -> å„ç§æ„ŸçŸ¥ä»»åŠ¡
            if (boundary_dataset / "degraded_images").exists():
                brightness_path = subject_path / "brightness_variation"
                self.copy_with_symlink(boundary_dataset / "degraded_images",
                                     brightness_path / "degraded_images")

        print("âœ… Subjectç±»åˆ«æ•°æ®æ•´ç†å®Œæˆ")

    def organize_relation_data(self):
        """æ•´ç†Relationç±»åˆ«æ•°æ®"""
        print("ğŸ”— æ•´ç†Relationç±»åˆ«æ•°æ®...")
        
        relation_path = self.output_path / "Relation"
        
        # åˆ›å»ºå ä½ç¬¦æ–‡ä»¶è¯´æ˜éœ€è¦çš„æ•°æ®
        for subcat_key, subcat_name in self.dataset_mapping["Relation"]["subcategories"].items():
            subcat_path = relation_path / subcat_key
            placeholder_file = subcat_path / "README.md"
            
            content = f"""# {subcat_name}

## æ•°æ®éœ€æ±‚è¯´æ˜
- **ç›®æ ‡**: {subcat_name}è¯„ä¼°æ•°æ®
- **çŠ¶æ€**: å¾…æ”¶é›†/ç”Ÿæˆ
- **å»ºè®®æ•°æ®æº**: 
  - Blenderç”Ÿæˆçš„3Dåœºæ™¯
  - SVGå‡ ä½•å›¾å½¢
  - æ‰‹å·¥æ ‡æ³¨çš„å…³ç³»æ•°æ®

## æ•°æ®æ ¼å¼è¦æ±‚
- å›¾ç‰‡æ ¼å¼: PNG/JPG
- æ ‡æ³¨æ ¼å¼: JSON
- å…³ç³»ç±»å‹: ç©ºé—´ä½ç½®ã€è·ç¦»ã€æ–¹å‘ã€æ¯”è¾ƒç­‰
"""
            
            with open(placeholder_file, 'w', encoding='utf-8') as f:
                f.write(content)
        
        print("âœ… Relationç±»åˆ«æ¡†æ¶åˆ›å»ºå®Œæˆï¼ˆéœ€è¦åç»­æ•°æ®æ”¶é›†ï¼‰")

    def organize_attribute_data(self):
        """æ•´ç†Attributeç±»åˆ«æ•°æ®"""
        print("ğŸ¨ æ•´ç†Attributeç±»åˆ«æ•°æ®...")
        
        attribute_path = self.output_path / "Attribute"
        
        # Real_World_Noise_Dataset çš„å™ªå£°å’Œåƒç´ æ“ä½œæ•°æ®
        noise_dataset = self.base_path / "Real_World_Noise_Dataset"
        if noise_dataset.exists():
            # å…¨å±€å™ªå£°
            if (noise_dataset / "gaussian_noise").exists():
                global_noise_path = attribute_path / "global_noise"
                self.copy_with_symlink(noise_dataset / "gaussian_noise",
                                     global_noise_path / "gaussian_noise")
            
            if (noise_dataset / "salt_pepper_noise").exists():
                global_noise_path = attribute_path / "global_noise"
                self.copy_with_symlink(noise_dataset / "salt_pepper_noise",
                                     global_noise_path / "salt_pepper_noise")
            
            # åƒç´ æ“ä½œ
            if (noise_dataset / "pixel_gradients").exists():
                pixel_path = attribute_path / "pixel_manipulation"
                self.copy_with_symlink(noise_dataset / "pixel_gradients",
                                     pixel_path / "pixelization")
            
            if (noise_dataset / "pixelation").exists():
                pixel_path = attribute_path / "pixel_manipulation"
                self.copy_with_symlink(noise_dataset / "pixelation",
                                     pixel_path / "pixelation_effects")

        print("âœ… Attributeç±»åˆ«æ•°æ®æ•´ç†å®Œæˆ")

    def organize_illusion_data(self):
        """æ•´ç†Illusionç±»åˆ«æ•°æ®"""
        print("ğŸ‘ï¸ æ•´ç†Illusionç±»åˆ«æ•°æ®...")
        
        illusion_path = self.output_path / "Illusion"
        
        # Unified_Illusion_Dataset åŒ…å«æ‰€æœ‰é”™è§‰æ•°æ®
        illusion_dataset = self.base_path / "Unified_Illusion_Dataset"
        if illusion_dataset.exists():
            # åˆæˆé”™è§‰æ•°æ®
            if (illusion_dataset / "Synthetic_Illusions").exists():
                synthetic_path = illusion_dataset / "Synthetic_Illusions"
                
                # å‡ ä½•é”™è§‰
                geometric_illusion_path = illusion_path / "geometric_illusions"
                self.copy_with_symlink(synthetic_path / "Geometric_Length_Illusions",
                                     geometric_illusion_path / "geometric_length")
                self.copy_with_symlink(synthetic_path / "Ambiguous_Figures_Illusions", 
                                     geometric_illusion_path / "ambiguous_figures")
                
                # è‰²å½©é”™è§‰
                color_illusion_path = illusion_path / "color_illusions"
                self.copy_with_symlink(synthetic_path / "Color_Brightness_Illusions",
                                     color_illusion_path / "color_brightness")
                
                # è¿åŠ¨é”™è§‰
                motion_illusion_path = illusion_path / "motion_illusions"
                self.copy_with_symlink(synthetic_path / "Grid_Motion_Illusions",
                                     motion_illusion_path / "grid_motion")
                
                # å…¶ä»–é”™è§‰
                misc_illusion_path = illusion_path / "ambiguous_figures"
                self.copy_with_symlink(synthetic_path / "Miscellaneous_Illusions",
                                     misc_illusion_path / "miscellaneous")
            
            # ç ”ç©¶æ•°æ®é›†
            if (illusion_dataset / "IllusionBench_Research_Dataset").exists():
                research_path = illusion_path / "research_benchmark"
                self.copy_with_symlink(illusion_dataset / "IllusionBench_Research_Dataset",
                                     research_path / "illusionbench_data")

        print("âœ… Illusionç±»åˆ«æ•°æ®æ•´ç†å®Œæˆ")

    def copy_with_symlink(self, source, target):
        """ä½¿ç”¨ç¬¦å·é“¾æ¥å¤åˆ¶æ•°æ®ï¼ˆèŠ‚çœç©ºé—´ï¼‰"""
        try:
            if source.exists():
                target.parent.mkdir(parents=True, exist_ok=True)
                if not target.exists():
                    # åˆ›å»ºç¬¦å·é“¾æ¥è€Œä¸æ˜¯å¤åˆ¶æ–‡ä»¶
                    target.symlink_to(source.resolve())
                    print(f"  ğŸ“ é“¾æ¥: {source.name} -> {target}")
        except Exception as e:
            print(f"  âŒ é“¾æ¥å¤±è´¥: {source} -> {target}, é”™è¯¯: {e}")

    def generate_comprehensive_readme(self):
        """ç”Ÿæˆç»¼åˆREADMEæ–‡æ¡£"""
        analysis = self.analyze_existing_datasets()
        
        readme_content = f"""# VLM Comprehensive Benchmark Dataset

## ğŸ¯ æ•°æ®é›†æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒæŒ‰ç…§å››å¤§æ ¸å¿ƒèƒ½åŠ›ç»´åº¦ç»„ç»‡ï¼š**Subjectï¼ˆä¸»ä½“æ„ŸçŸ¥ï¼‰**ã€**Relationï¼ˆå…³ç³»ç†è§£ï¼‰**ã€**Attributeï¼ˆå±æ€§æ„ŸçŸ¥ï¼‰**ã€**Illusionï¼ˆé”™è§‰æ„ŸçŸ¥ï¼‰**ã€‚

### ğŸ“Š æ•°æ®é›†ç»Ÿè®¡

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

"""
        
        for category, info in analysis["categories"].items():
            readme_content += f"""
## {category} - {info["description"]}

### å­ç±»åˆ«ï¼š
"""
            for subcat_key, subcat_name in info["subcategories"].items():
                readme_content += f"- **{subcat_key}**: {subcat_name}\n"
            
            if info["datasets"]:
                readme_content += f"\n### åŒ…å«æ•°æ®é›†ï¼š\n"
                for dataset in info["datasets"]:
                    readme_content += f"""
- **{dataset["name"]}**
  - å›¾ç‰‡æ•°é‡: {dataset["total_images"]:,}
  - ç›®å½•æ•°é‡: {dataset["total_directories"]}
  - æ•°æ®å¤§å°: {dataset["size_mb"]} MB
"""

        readme_content += f"""

## ğŸ“ ç›®å½•ç»“æ„

```
VLM_Comprehensive_Benchmark/
â”œâ”€â”€ Subject/                    # ä¸»ä½“æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼°
â”‚   â”œâ”€â”€ clarity_degradation/    # æ¸…æ™°åº¦é€€åŒ–æ„ŸçŸ¥
â”‚   â”œâ”€â”€ brightness_variation/   # äº®åº¦å˜åŒ–æ„ŸçŸ¥
â”‚   â”œâ”€â”€ contrast_variation/     # å¯¹æ¯”åº¦å˜åŒ–æ„ŸçŸ¥
â”‚   â”œâ”€â”€ color_distortion/       # é¢œè‰²å¤±çœŸæ„ŸçŸ¥
â”‚   â”œâ”€â”€ color_shift/           # è‰²åè¯†åˆ«
â”‚   â”œâ”€â”€ fine_grained_classification/  # ç»†ç²’åº¦ä¸»ä½“åˆ†ç±»
â”‚   â”œâ”€â”€ colorblind_recognition/ # è‰²ç›²è¯†åˆ«
â”‚   â””â”€â”€ resolution_variation/   # åˆ†è¾¨ç‡å˜åŒ–
â”‚
â”œâ”€â”€ Relation/                   # å…³ç³»ç†è§£èƒ½åŠ›è¯„ä¼°
â”‚   â”œâ”€â”€ spatial_relations/      # ç©ºé—´ä½ç½®å…³ç³»
â”‚   â”œâ”€â”€ proximity_relations/    # è·ç¦»/é è¿‘å…³ç³»
â”‚   â”œâ”€â”€ alignment_relations/    # å¯¹é½/æ–¹å‘å…³ç³»
â”‚   â””â”€â”€ comparative_relations/  # æ¯”è¾ƒå…³ç³»
â”‚
â”œâ”€â”€ Attribute/                  # å±æ€§æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼°
â”‚   â”œâ”€â”€ global_noise/          # å›¾åƒæ•´ä½“åŠ å™ªå£°
â”‚   â””â”€â”€ pixel_manipulation/    # åƒç´ ç‚¹æ“ä½œ
â”‚
â”œâ”€â”€ Illusion/                   # é”™è§‰æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼°
â”‚   â”œâ”€â”€ geometric_illusions/    # å‡ ä½•é”™è§‰
â”‚   â”œâ”€â”€ color_illusions/       # è‰²å½©é”™è§‰
â”‚   â”œâ”€â”€ motion_illusions/      # è¿åŠ¨é”™è§‰
â”‚   â””â”€â”€ ambiguous_figures/     # æ¨¡ç³Šå›¾å½¢
â”‚
â”œâ”€â”€ README.md                   # æœ¬æ–‡æ¡£
â”œâ”€â”€ dataset_analysis.json      # æ•°æ®é›†åˆ†ææŠ¥å‘Š
â””â”€â”€ organize_benchmark.py      # æ•°æ®æ•´ç†è„šæœ¬
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ•°æ®åŠ è½½
```python
from pathlib import Path
import json

# åŠ è½½æ•°æ®é›†
benchmark_path = Path("VLM_Comprehensive_Benchmark")

# åŠ è½½Subjectç±»åˆ«æ•°æ®
subject_data = benchmark_path / "Subject"
clarity_data = subject_data / "clarity_degradation"

# åŠ è½½Illusionç±»åˆ«æ•°æ®  
illusion_data = benchmark_path / "Illusion"
geometric_illusions = illusion_data / "geometric_illusions"
```

### è¯„ä¼°æŒ‡æ ‡

#### Subjectç±»åˆ«
- ä¸»ä½“è¯†åˆ«å‡†ç¡®ç‡
- å±æ€§æè¿°å‡†ç¡®æ€§
- é€€åŒ–æ¡ä»¶ä¸‹çš„é²æ£’æ€§

#### Relationç±»åˆ«
- ç©ºé—´å…³ç³»åˆ¤æ–­å‡†ç¡®ç‡
- ç›¸å¯¹ä½ç½®æè¿°å‡†ç¡®æ€§
- æ¯”è¾ƒå…³ç³»ç†è§£èƒ½åŠ›

#### Attributeç±»åˆ«
- å…¨å±€å±æ€§æ„ŸçŸ¥èƒ½åŠ›
- å±€éƒ¨ç»†èŠ‚è¯†åˆ«èƒ½åŠ›
- å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½

#### Illusionç±»åˆ«
- é”™è§‰è¯†åˆ«å‡†ç¡®ç‡
- é”™è§‰è§£é‡Šåˆç†æ€§
- è§†è§‰æœºåˆ¶ç†è§£æ·±åº¦

## ğŸ“ˆ æ•°æ®é›†ç‰¹ç‚¹

1. **å¤šæ¨¡æ€è¦†ç›–**: æ¶µç›–å‡ ä½•ã€è‰²å½©ã€è¿åŠ¨ã€ç©ºé—´ç­‰å¤šä¸ªè§†è§‰ç»´åº¦
2. **æ¢¯åº¦å˜åŒ–**: æ¯ç§æ•ˆæœéƒ½æœ‰100ä¸ªå¼ºåº¦æ¢¯åº¦ï¼Œæ”¯æŒç»†ç²’åº¦è¯„ä¼°
3. **çœŸå®åœºæ™¯**: åŸºäºçœŸå®ä¸–ç•Œå›¾ç‰‡ç”Ÿæˆï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼
4. **æ ‡å‡†åŒ–æ ¼å¼**: ç»Ÿä¸€çš„PNGæ ¼å¼å’ŒJSONå…ƒæ•°æ®
5. **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ·»åŠ æ–°çš„æµ‹è¯•ç±»åˆ«

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

- **å›¾ç‰‡æ ¼å¼**: PNG (æ— æŸå‹ç¼©)
- **åˆ†è¾¨ç‡**: æ ¹æ®åŸå§‹æ•°æ®é›†ä¿æŒ
- **è‰²å½©ç©ºé—´**: RGB
- **å…ƒæ•°æ®**: JSONæ ¼å¼ï¼ŒåŒ…å«ç”Ÿæˆå‚æ•°å’Œæ ‡æ³¨ä¿¡æ¯

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æ­¤æ•°æ®é›†ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@dataset{{vlm_comprehensive_benchmark,
  title={{VLM Comprehensive Benchmark Dataset}},
  year={{2025}},
  month={{08}},
  note={{Generated dataset for comprehensive VLM evaluation}}
}}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›æ•°æ®é›†è´¨é‡æˆ–æ·»åŠ æ–°çš„æµ‹è¯•ç±»åˆ«ã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬æ•°æ®é›†ä»…ä¾›ç ”ç©¶ä½¿ç”¨ã€‚

---
*æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d")}*
"""

        # ä¿å­˜README
        with open(self.output_path / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        # ä¿å­˜åˆ†ææ•°æ®
        with open(self.output_path / "dataset_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        print("âœ… READMEå’Œåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")

    def organize_all(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ•´ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹VLMç»¼åˆåŸºå‡†æ•°æ®é›†æ•´ç†")
        print("=" * 60)
        
        self.organize_subject_data()
        self.organize_relation_data() 
        self.organize_attribute_data()
        self.organize_illusion_data()
        self.generate_comprehensive_readme()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ VLMç»¼åˆåŸºå‡†æ•°æ®é›†æ•´ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
        print("ğŸ“– æŸ¥çœ‹README.mdäº†è§£è¯¦ç»†ä¿¡æ¯")

def main():
    organizer = VLMBenchmarkOrganizer()
    organizer.organize_all()

if __name__ == "__main__":
    main()