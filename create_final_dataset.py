#!/usr/bin/env python3
"""
åˆ›å»ºæœ€ç»ˆé«˜è´¨é‡å®Œæ•´çš„VLMç»¼åˆåŸºå‡†æ•°æ®é›†
ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡æ ·æœ¬
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import random

class FinalDatasetCreator:
    def __init__(self):
        self.base_path = Path("/home/jgy")
        self.final_dataset_path = self.base_path / "VLM_Final_Benchmark_Dataset"
        self.source_datasets = {
            "Real_World_Noise_Dataset": self.base_path / "Real_World_Noise_Dataset",
            "Unified_Illusion_Dataset": self.base_path / "Unified_Illusion_Dataset", 
            "visual_boundary_dataset": self.base_path / "visual_boundary_dataset",
            "VLM_Comprehensive_Benchmark": self.base_path / "VLM_Comprehensive_Benchmark"
        }
        
        # æœ€ç»ˆæ•°æ®é›†æ ‡å‡†
        self.quality_standards = {
            "min_samples_per_category": 50,
            "min_image_resolution": (512, 512),
            "max_file_size_mb": 10,
            "required_metadata": ["description", "labels", "difficulty"]
        }
        
        print("ğŸ¯ åˆ›å»ºæœ€ç»ˆé«˜è´¨é‡VLMç»¼åˆåŸºå‡†æ•°æ®é›†")
        print("=" * 60)

    def setup_final_structure(self):
        """è®¾ç½®æœ€ç»ˆæ•°æ®é›†ç»“æ„"""
        print("ğŸ“ åˆ›å»ºæœ€ç»ˆæ•°æ®é›†ç»“æ„...")
        
        # æ¸…ç†å¹¶åˆ›å»ºç›®å½•
        if self.final_dataset_path.exists():
            shutil.rmtree(self.final_dataset_path)
        self.final_dataset_path.mkdir()
        
        # å››å¤§ç±»åˆ«ç»“æ„
        categories = {
            "Subject": {
                "description": "ä¸»ä½“æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° - VLMå¯¹å›¾åƒä¸­ä¸»ä½“çš„è¯†åˆ«ã€æ„ŸçŸ¥å’Œå±æ€§ç†è§£",
                "subcategories": [
                    "clarity_degradation",     # æ¸…æ™°åº¦é€€åŒ–æ„ŸçŸ¥
                    "brightness_variation",    # äº®åº¦å˜åŒ–æ„ŸçŸ¥
                    "contrast_variation",      # å¯¹æ¯”åº¦å˜åŒ–æ„ŸçŸ¥
                    "color_distortion",        # é¢œè‰²å¤±çœŸæ„ŸçŸ¥
                    "color_shift",            # è‰²åè¯†åˆ«
                    "fine_grained_classification", # ç»†ç²’åº¦ä¸»ä½“åˆ†ç±»
                    "resolution_variation"     # åˆ†è¾¨ç‡å˜åŒ–
                ]
            },
            "Relation": {
                "description": "å…³ç³»ç†è§£èƒ½åŠ›è¯„ä¼° - VLMå¯¹å›¾åƒä¸­å¤šä¸ªä¸»ä½“ä¹‹é—´å…³ç³»çš„ç†è§£",
                "subcategories": [
                    "spatial_relations",      # ç©ºé—´ä½ç½®å…³ç³»
                    "proximity_relations",    # è·ç¦»/é è¿‘å…³ç³»
                    "alignment_relations",    # å¯¹é½/æ–¹å‘å…³ç³»
                    "comparative_relations"   # æ¯”è¾ƒå…³ç³»
                ]
            },
            "Attribute": {
                "description": "å±æ€§æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° - VLMå¯¹å›¾åƒå±æ€§çš„æ„ŸçŸ¥å’Œç†è§£",
                "subcategories": [
                    "global_noise",           # å›¾åƒæ•´ä½“åŠ å™ªå£°
                    "pixel_manipulation",     # åƒç´ ç‚¹æ“ä½œ
                    "texture_analysis",       # çº¹ç†åˆ†æ
                    "pattern_recognition"     # æ¨¡å¼è¯†åˆ«
                ]
            },
            "Illusion": {
                "description": "é”™è§‰æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° - VLMå¯¹è§†è§‰é”™è§‰çš„æ„ŸçŸ¥å’Œç†è§£",
                "subcategories": [
                    "geometric_illusions",    # å‡ ä½•é”™è§‰
                    "color_illusions",       # è‰²å½©é”™è§‰
                    "motion_illusions",      # è¿åŠ¨é”™è§‰
                    "ambiguous_figures"      # æ¨¡ç³Šå›¾å½¢
                ]
            }
        }
        
        for category, info in categories.items():
            category_path = self.final_dataset_path / category
            category_path.mkdir()
            
            # åˆ›å»ºç±»åˆ«ä¿¡æ¯æ–‡ä»¶
            category_info = {
                "category": category,
                "description": info["description"],
                "subcategories": info["subcategories"],
                "creation_date": datetime.now().isoformat(),
                "quality_standard": self.quality_standards
            }
            
            with open(category_path / "category_info.json", 'w', encoding='utf-8') as f:
                json.dump(category_info, f, indent=2, ensure_ascii=False)
            
            # ä¸ºæ¯ä¸ªå­ç±»åˆ«åˆ›å»ºç›®å½•
            for subcat in info["subcategories"]:
                subcat_path = category_path / subcat
                subcat_path.mkdir()
                (subcat_path / "images").mkdir()
                (subcat_path / "metadata").mkdir()
        
        print("âœ… æœ€ç»ˆæ•°æ®é›†ç»“æ„åˆ›å»ºå®Œæˆ")

    def curate_subject_data(self):
        """ç²¾é€‰Subjectç±»åˆ«æ•°æ®"""
        print("\nğŸ­ ç²¾é€‰Subjectç±»åˆ«æ•°æ®...")
        
        subject_path = self.final_dataset_path / "Subject"
        curated_count = 0
        
        # 1. æ¸…æ™°åº¦é€€åŒ– - ä»å™ªå£°å’Œæ¨¡ç³Šæ•°æ®ä¸­é€‰æ‹©æœ€å¥½çš„50ä¸ª
        clarity_source = self.source_datasets["Real_World_Noise_Dataset"]
        if clarity_source.exists():
            clarity_samples = self.select_best_clarity_samples(clarity_source, 60)
            self.copy_samples_to_category(clarity_samples, subject_path / "clarity_degradation")
            curated_count += len(clarity_samples)
            print(f"  âœ… æ¸…æ™°åº¦é€€åŒ–: {len(clarity_samples)} ä¸ªæ ·æœ¬")
        
        # 2. äº®åº¦å˜åŒ– - ä»visual_boundary_datasetçš„é€€åŒ–å›¾ç‰‡ä¸­é€‰æ‹©
        boundary_source = self.source_datasets["visual_boundary_dataset"]
        if (boundary_source / "degraded_images/brightness").exists():
            brightness_samples = self.select_brightness_samples(boundary_source, 55)
            self.copy_samples_to_category(brightness_samples, subject_path / "brightness_variation")
            curated_count += len(brightness_samples)
            print(f"  âœ… äº®åº¦å˜åŒ–: {len(brightness_samples)} ä¸ªæ ·æœ¬")
        
        # 3. å¯¹æ¯”åº¦å˜åŒ– - ç”Ÿæˆæ–°çš„é«˜è´¨é‡å¯¹æ¯”åº¦å˜åŒ–æ ·æœ¬
        contrast_samples = self.generate_contrast_samples(50)
        self.save_generated_samples(contrast_samples, subject_path / "contrast_variation")
        curated_count += len(contrast_samples)
        print(f"  âœ… å¯¹æ¯”åº¦å˜åŒ–: {len(contrast_samples)} ä¸ªæ ·æœ¬")
        
        # 4. é¢œè‰²å¤±çœŸ - åŸºäºç°æœ‰å›¾ç‰‡ç”Ÿæˆè‰²å½©å˜åŒ–
        color_samples = self.generate_color_distortion_samples(50)
        self.save_generated_samples(color_samples, subject_path / "color_distortion")
        curated_count += len(color_samples)
        print(f"  âœ… é¢œè‰²å¤±çœŸ: {len(color_samples)} ä¸ªæ ·æœ¬")
        
        # 5. è‰²åè¯†åˆ« - è‰²ç›¸åç§»æ ·æœ¬
        shift_samples = self.generate_color_shift_samples(50)
        self.save_generated_samples(shift_samples, subject_path / "color_shift")
        curated_count += len(shift_samples)
        print(f"  âœ… è‰²åè¯†åˆ«: {len(shift_samples)} ä¸ªæ ·æœ¬")
        
        # 6. ç»†ç²’åº¦åˆ†ç±» - ä»visual_boundary_dataseté€‰æ‹©å¤šæ ·åŒ–å›¾ç‰‡
        fine_samples = self.select_fine_grained_samples(boundary_source, 65)
        self.copy_samples_to_category(fine_samples, subject_path / "fine_grained_classification")
        curated_count += len(fine_samples)
        print(f"  âœ… ç»†ç²’åº¦åˆ†ç±»: {len(fine_samples)} ä¸ªæ ·æœ¬")
        
        # 7. åˆ†è¾¨ç‡å˜åŒ– - ç”Ÿæˆå¤šåˆ†è¾¨ç‡å˜åŒ–æ ·æœ¬
        resolution_samples = self.generate_resolution_samples(50)
        self.save_generated_samples(resolution_samples, subject_path / "resolution_variation")
        curated_count += len(resolution_samples)
        print(f"  âœ… åˆ†è¾¨ç‡å˜åŒ–: {len(resolution_samples)} ä¸ªæ ·æœ¬")
        
        return curated_count

    def curate_relation_data(self):
        """ç²¾é€‰Relationç±»åˆ«æ•°æ®"""
        print("\nğŸ”— ç²¾é€‰Relationç±»åˆ«æ•°æ®...")
        
        relation_path = self.final_dataset_path / "Relation"
        curated_count = 0
        
        # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„å…³ç³»æ•°æ®æ¨¡æ¿ï¼Œä½†è¡¥å……å®é™…å›¾ç‰‡
        relation_source = self.source_datasets["VLM_Comprehensive_Benchmark"] / "Relation"
        
        relation_categories = [
            "spatial_relations", 
            "proximity_relations", 
            "alignment_relations", 
            "comparative_relations"
        ]
        
        for rel_cat in relation_categories:
            # ä»visual_boundary_datasetä¸­æ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡
            matched_samples = self.match_relation_samples_with_images(rel_cat, 55)
            if matched_samples:
                target_path = relation_path / rel_cat
                self.save_relation_samples(matched_samples, target_path)
                curated_count += len(matched_samples)
                print(f"  âœ… {rel_cat}: {len(matched_samples)} ä¸ªæ ·æœ¬")
        
        return curated_count

    def curate_attribute_data(self):
        """ç²¾é€‰Attributeç±»åˆ«æ•°æ®"""
        print("\nğŸ¨ ç²¾é€‰Attributeç±»åˆ«æ•°æ®...")
        
        attribute_path = self.final_dataset_path / "Attribute"
        curated_count = 0
        
        # 1. å…¨å±€å™ªå£° - é€‰æ‹©æœ€ä½³å™ªå£°æ ·æœ¬
        noise_source = self.source_datasets["Real_World_Noise_Dataset"]
        if (noise_source / "gaussian_noise").exists():
            noise_samples = self.select_best_noise_samples(noise_source, 60)
            self.copy_samples_to_category(noise_samples, attribute_path / "global_noise")
            curated_count += len(noise_samples)
            print(f"  âœ… å…¨å±€å™ªå£°: {len(noise_samples)} ä¸ªæ ·æœ¬")
        
        # 2. åƒç´ æ“ä½œ - åƒç´ åŒ–æ•ˆæœ
        if (noise_source / "pixel_gradients").exists():
            pixel_samples = self.select_best_pixel_samples(noise_source, 55)
            self.copy_samples_to_category(pixel_samples, attribute_path / "pixel_manipulation")
            curated_count += len(pixel_samples)
            print(f"  âœ… åƒç´ æ“ä½œ: {len(pixel_samples)} ä¸ªæ ·æœ¬")
        
        # 3. çº¹ç†åˆ†æ - æ–°ç”Ÿæˆçº¹ç†å˜åŒ–æ ·æœ¬
        texture_samples = self.generate_texture_samples(50)
        self.save_generated_samples(texture_samples, attribute_path / "texture_analysis")
        curated_count += len(texture_samples)
        print(f"  âœ… çº¹ç†åˆ†æ: {len(texture_samples)} ä¸ªæ ·æœ¬")
        
        # 4. æ¨¡å¼è¯†åˆ« - ç”Ÿæˆå‡ ä½•æ¨¡å¼æ ·æœ¬
        pattern_samples = self.generate_pattern_samples(50)
        self.save_generated_samples(pattern_samples, attribute_path / "pattern_recognition")
        curated_count += len(pattern_samples)
        print(f"  âœ… æ¨¡å¼è¯†åˆ«: {len(pattern_samples)} ä¸ªæ ·æœ¬")
        
        return curated_count

    def curate_illusion_data(self):
        """ç²¾é€‰Illusionç±»åˆ«æ•°æ®"""
        print("\nğŸ‘ï¸ ç²¾é€‰Illusionç±»åˆ«æ•°æ®...")
        
        illusion_path = self.final_dataset_path / "Illusion"
        curated_count = 0
        
        illusion_source = self.source_datasets["Unified_Illusion_Dataset"]
        if illusion_source.exists():
            synthetic_path = illusion_source / "Synthetic_Illusions"
            
            # 1. å‡ ä½•é”™è§‰ - é€‰æ‹©æœ€ä½³å‡ ä½•é”™è§‰
            if (synthetic_path / "Geometric_Length_Illusions").exists():
                geometric_samples = self.select_best_illusion_samples(
                    synthetic_path / "Geometric_Length_Illusions", 70)
                self.copy_samples_to_category(geometric_samples, illusion_path / "geometric_illusions")
                curated_count += len(geometric_samples)
                print(f"  âœ… å‡ ä½•é”™è§‰: {len(geometric_samples)} ä¸ªæ ·æœ¬")
            
            # 2. è‰²å½©é”™è§‰
            if (synthetic_path / "Color_Brightness_Illusions").exists():
                color_samples = self.select_best_illusion_samples(
                    synthetic_path / "Color_Brightness_Illusions", 60)
                self.copy_samples_to_category(color_samples, illusion_path / "color_illusions")
                curated_count += len(color_samples)
                print(f"  âœ… è‰²å½©é”™è§‰: {len(color_samples)} ä¸ªæ ·æœ¬")
            
            # 3. è¿åŠ¨é”™è§‰
            if (synthetic_path / "Grid_Motion_Illusions").exists():
                motion_samples = self.select_best_illusion_samples(
                    synthetic_path / "Grid_Motion_Illusions", 55)
                self.copy_samples_to_category(motion_samples, illusion_path / "motion_illusions")
                curated_count += len(motion_samples)
                print(f"  âœ… è¿åŠ¨é”™è§‰: {len(motion_samples)} ä¸ªæ ·æœ¬")
            
            # 4. æ¨¡ç³Šå›¾å½¢
            if (synthetic_path / "Ambiguous_Figures_Illusions").exists():
                ambiguous_samples = self.select_best_illusion_samples(
                    synthetic_path / "Ambiguous_Figures_Illusions", 50)
                self.copy_samples_to_category(ambiguous_samples, illusion_path / "ambiguous_figures")
                curated_count += len(ambiguous_samples)
                print(f"  âœ… æ¨¡ç³Šå›¾å½¢: {len(ambiguous_samples)} ä¸ªæ ·æœ¬")
        
        return curated_count

    def select_best_clarity_samples(self, source_path, target_count):
        """é€‰æ‹©æœ€ä½³æ¸…æ™°åº¦æ ·æœ¬"""
        samples = []
        
        # ä»å™ªå£°æ¢¯åº¦ä¸­é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
        noise_path = source_path / "noise_gradients"
        if noise_path.exists():
            for img_dir in list(noise_path.iterdir())[:10]:  # å‰10ä¸ªå›¾ç‰‡ç»„
                if img_dir.is_dir():
                    # é€‰æ‹©ä¸åŒå¼ºåº¦çº§åˆ«çš„å™ªå£°
                    levels = [0, 25, 50, 75, 99]  # 5ä¸ªä¸åŒå¼ºåº¦
                    for level in levels:
                        img_file = img_dir / f"noise_{level:03d}.png"
                        if img_file.exists() and len(samples) < target_count:
                            samples.append({
                                "image_path": str(img_file),
                                "source_image": img_dir.name,
                                "degradation_type": "gaussian_noise",
                                "intensity": level / 99.0,
                                "description": f"Gaussian noise level {level}",
                                "difficulty": "medium" if 25 <= level <= 75 else "high"
                            })
        
        return samples[:target_count]

    def select_brightness_samples(self, source_path, target_count):
        """é€‰æ‹©äº®åº¦å˜åŒ–æ ·æœ¬"""
        samples = []
        brightness_path = source_path / "degraded_images/brightness"
        
        if brightness_path.exists():
            for img_file in list(brightness_path.glob("*.jpg"))[:target_count]:
                # ä»æ–‡ä»¶åè§£æäº®åº¦çº§åˆ«
                filename = img_file.name
                if "brightness_level" in filename:
                    level_str = filename.split("brightness_level_")[1].split(".")[0]
                    try:
                        level = int(level_str)
                        samples.append({
                            "image_path": str(img_file),
                            "degradation_type": "brightness_change",
                            "brightness_level": level,
                            "intensity": level / 100.0,
                            "description": f"Brightness variation level {level}",
                            "difficulty": "easy" if level < 30 or level > 70 else "medium"
                        })
                    except:
                        pass
        
        return samples[:target_count]

    def generate_contrast_samples(self, target_count):
        """ç”Ÿæˆå¯¹æ¯”åº¦å˜åŒ–æ ·æœ¬"""
        samples = []
        
        # åŸºäºvisual_boundary_datasetçš„å›¾ç‰‡ç”Ÿæˆå¯¹æ¯”åº¦å˜åŒ–
        base_images = list((self.source_datasets["visual_boundary_dataset"] / "downloaded_images").glob("*.jpg"))[:10]
        
        for i, base_img in enumerate(base_images):
            if len(samples) >= target_count:
                break
                
            # ä¸ºæ¯å¼ åŸºç¡€å›¾ç‰‡ç”Ÿæˆ5ä¸ªä¸åŒå¯¹æ¯”åº¦çº§åˆ«
            for contrast_level in [0.3, 0.6, 1.0, 1.5, 2.0]:
                if len(samples) >= target_count:
                    break
                    
                samples.append({
                    "base_image": str(base_img),
                    "effect_type": "contrast_adjustment",
                    "contrast_factor": contrast_level,
                    "description": f"Contrast adjustment factor {contrast_level}",
                    "difficulty": "easy" if contrast_level == 1.0 else "medium",
                    "parameters": {"contrast": contrast_level}
                })
        
        return samples[:target_count]

    def generate_color_distortion_samples(self, target_count):
        """ç”Ÿæˆé¢œè‰²å¤±çœŸæ ·æœ¬"""
        samples = []
        
        distortion_types = [
            {"type": "saturation", "values": [0.0, 0.5, 1.0, 1.5, 2.0]},
            {"type": "hue_shift", "values": [0, 30, 60, 90, 120]},
            {"type": "color_balance", "values": [(1.2, 1.0, 0.8), (0.8, 1.0, 1.2), (1.0, 1.2, 0.8)]}
        ]
        
        base_images = list((self.source_datasets["visual_boundary_dataset"] / "downloaded_images").glob("*.jpg"))[:15]
        
        sample_id = 1
        for base_img in base_images:
            for distortion in distortion_types:
                for value in distortion["values"]:
                    if len(samples) >= target_count:
                        break
                        
                    samples.append({
                        "sample_id": f"color_dist_{sample_id:03d}",
                        "base_image": str(base_img),
                        "distortion_type": distortion["type"],
                        "distortion_value": value,
                        "description": f"{distortion['type']} distortion: {value}",
                        "difficulty": "medium"
                    })
                    sample_id += 1
                    
                if len(samples) >= target_count:
                    break
            if len(samples) >= target_count:
                break
        
        return samples[:target_count]

    def generate_color_shift_samples(self, target_count):
        """ç”Ÿæˆè‰²åè¯†åˆ«æ ·æœ¬"""
        samples = []
        
        color_shifts = [
            {"name": "red_shift", "hue_offset": 0, "intensity": [0.2, 0.5, 0.8]},
            {"name": "green_shift", "hue_offset": 120, "intensity": [0.2, 0.5, 0.8]},
            {"name": "blue_shift", "hue_offset": 240, "intensity": [0.2, 0.5, 0.8]},
            {"name": "yellow_shift", "hue_offset": 60, "intensity": [0.2, 0.5, 0.8]},
            {"name": "cyan_shift", "hue_offset": 180, "intensity": [0.2, 0.5, 0.8]}
        ]
        
        base_images = list((self.source_datasets["visual_boundary_dataset"] / "downloaded_images").glob("*.jpg"))[:12]
        
        sample_id = 1
        for base_img in base_images:
            for shift in color_shifts:
                for intensity in shift["intensity"]:
                    if len(samples) >= target_count:
                        break
                        
                    samples.append({
                        "sample_id": f"color_shift_{sample_id:03d}",
                        "base_image": str(base_img),
                        "shift_type": shift["name"],
                        "hue_offset": shift["hue_offset"],
                        "shift_intensity": intensity,
                        "description": f"{shift['name']} with intensity {intensity}",
                        "difficulty": "easy" if intensity < 0.4 else "hard"
                    })
                    sample_id += 1
        
        return samples[:target_count]

    def select_fine_grained_samples(self, source_path, target_count):
        """é€‰æ‹©ç»†ç²’åº¦åˆ†ç±»æ ·æœ¬"""
        samples = []
        
        # ä»downloaded_imagesä¸­é€‰æ‹©ä¸åŒç±»å‹çš„å›¾ç‰‡
        images_path = source_path / "downloaded_images"
        if images_path.exists():
            image_categories = {
                "fruits": ["apple", "banana", "cherry", "grape", "mango", "strawberry"],
                "landscapes": ["landscape", "ocean", "mountain"],
                "portraits": ["portrait", "face"],
                "objects": ["chair", "bread", "bear"],
                "scenes": ["street", "bird", "dance"]
            }
            
            for category, keywords in image_categories.items():
                for keyword in keywords:
                    matching_images = list(images_path.glob(f"*{keyword}*"))
                    for img in matching_images:
                        if len(samples) >= target_count:
                            break
                            
                        samples.append({
                            "image_path": str(img),
                            "category": category,
                            "subcategory": keyword,
                            "classification_difficulty": "fine_grained",
                            "description": f"{category} - {keyword} classification",
                            "difficulty": "medium"
                        })
        
        return samples[:target_count]

    def generate_resolution_samples(self, target_count):
        """ç”Ÿæˆåˆ†è¾¨ç‡å˜åŒ–æ ·æœ¬"""
        samples = []
        
        resolutions = [
            {"name": "very_low", "size": (128, 128), "scale": 0.25},
            {"name": "low", "size": (256, 256), "scale": 0.5}, 
            {"name": "medium", "size": (512, 512), "scale": 1.0},
            {"name": "high", "size": (1024, 1024), "scale": 2.0}
        ]
        
        base_images = list((self.source_datasets["visual_boundary_dataset"] / "downloaded_images").glob("*.jpg"))[:15]
        
        sample_id = 1
        for base_img in base_images:
            for res in resolutions:
                if len(samples) >= target_count:
                    break
                    
                samples.append({
                    "sample_id": f"resolution_{sample_id:03d}",
                    "base_image": str(base_img),
                    "target_resolution": res["size"],
                    "scale_factor": res["scale"],
                    "resolution_name": res["name"],
                    "description": f"Resolution: {res['size'][0]}x{res['size'][1]}",
                    "difficulty": "easy" if res["scale"] >= 0.5 else "hard"
                })
                sample_id += 1
        
        return samples[:target_count]

    def match_relation_samples_with_images(self, relation_type, target_count):
        """ä¸ºå…³ç³»æ•°æ®åŒ¹é…çœŸå®å›¾ç‰‡"""
        samples = []
        
        # ä½¿ç”¨visual_boundary_datasetä¸­çš„å›¾ç‰‡
        images_path = self.source_datasets["visual_boundary_dataset"] / "downloaded_images"
        available_images = list(images_path.glob("*.jpg"))[:20]
        
        # ä¸ºæ¯ç§å…³ç³»ç±»å‹ç”Ÿæˆæ ·æœ¬
        relation_templates = {
            "spatial_relations": [
                {"type": "on_top_of", "description": "Object A is on top of object B"},
                {"type": "beside", "description": "Object A is beside object B"},
                {"type": "in_front_of", "description": "Object A is in front of object B"},
                {"type": "behind", "description": "Object A is behind object B"}
            ],
            "proximity_relations": [
                {"type": "close_to", "description": "Objects are close to each other"},
                {"type": "far_from", "description": "Objects are far from each other"},
                {"type": "adjacent", "description": "Objects are adjacent"}
            ],
            "alignment_relations": [
                {"type": "aligned", "description": "Objects are aligned"},
                {"type": "parallel", "description": "Objects are parallel"},
                {"type": "perpendicular", "description": "Objects are perpendicular"}
            ],
            "comparative_relations": [
                {"type": "bigger_than", "description": "Object A is bigger than object B"},
                {"type": "smaller_than", "description": "Object A is smaller than object B"},
                {"type": "brighter_than", "description": "Object A is brighter than object B"}
            ]
        }
        
        templates = relation_templates.get(relation_type, [])
        
        sample_id = 1
        for img in available_images:
            for template in templates:
                if len(samples) >= target_count:
                    break
                    
                samples.append({
                    "sample_id": f"{relation_type}_{sample_id:03d}",
                    "image_path": str(img),
                    "relation_type": relation_type,
                    "relation_subtype": template["type"],
                    "description": template["description"],
                    "difficulty": "medium",
                    "status": "requires_annotation"
                })
                sample_id += 1
        
        return samples[:target_count]

    def select_best_noise_samples(self, source_path, target_count):
        """é€‰æ‹©æœ€ä½³å™ªå£°æ ·æœ¬"""
        return self.select_best_clarity_samples(source_path, target_count)

    def select_best_pixel_samples(self, source_path, target_count):
        """é€‰æ‹©æœ€ä½³åƒç´ æ“ä½œæ ·æœ¬"""
        samples = []
        
        pixel_path = source_path / "pixel_gradients"
        if pixel_path.exists():
            for img_dir in list(pixel_path.iterdir())[:12]:
                if img_dir.is_dir():
                    # é€‰æ‹©ä¸åŒåƒç´ åŒ–çº§åˆ«
                    levels = [0, 20, 40, 60, 80, 99]
                    for level in levels:
                        img_file = img_dir / f"pixel_{level:03d}.png"
                        if img_file.exists() and len(samples) < target_count:
                            samples.append({
                                "image_path": str(img_file),
                                "source_image": img_dir.name,
                                "effect_type": "pixelization",
                                "pixel_level": level,
                                "intensity": level / 99.0,
                                "description": f"Pixelization level {level}",
                                "difficulty": "easy" if level < 40 else "medium"
                            })
        
        return samples[:target_count]

    def generate_texture_samples(self, target_count):
        """ç”Ÿæˆçº¹ç†åˆ†ææ ·æœ¬"""
        samples = []
        
        texture_types = ["smooth", "rough", "granular", "fibrous", "crystalline"]
        base_images = list((self.source_datasets["visual_boundary_dataset"] / "downloaded_images").glob("*.jpg"))[:12]
        
        sample_id = 1
        for base_img in base_images:
            for texture in texture_types:
                if len(samples) >= target_count:
                    break
                    
                samples.append({
                    "sample_id": f"texture_{sample_id:03d}",
                    "base_image": str(base_img),
                    "texture_type": texture,
                    "description": f"Texture analysis: {texture} surface",
                    "difficulty": "medium"
                })
                sample_id += 1
        
        return samples[:target_count]

    def generate_pattern_samples(self, target_count):
        """ç”Ÿæˆæ¨¡å¼è¯†åˆ«æ ·æœ¬"""
        samples = []
        
        patterns = ["stripes", "dots", "grids", "waves", "spirals", "checkerboard"]
        
        sample_id = 1
        for pattern in patterns:
            for variant in range(target_count // len(patterns) + 1):
                if len(samples) >= target_count:
                    break
                    
                samples.append({
                    "sample_id": f"pattern_{sample_id:03d}",
                    "pattern_type": pattern,
                    "variant": variant,
                    "description": f"Pattern recognition: {pattern} pattern variant {variant}",
                    "difficulty": "medium"
                })
                sample_id += 1
        
        return samples[:target_count]

    def select_best_illusion_samples(self, source_path, target_count):
        """é€‰æ‹©æœ€ä½³é”™è§‰æ ·æœ¬"""
        samples = []
        
        if source_path.exists():
            # ä»æ¯ä¸ªé”™è§‰ç±»å‹ä¸­é€‰æ‹©æœ€ä½³æ ·æœ¬
            for illusion_dir in source_path.iterdir():
                if illusion_dir.is_dir():
                    gradient_path = illusion_dir / "gradients"
                    if gradient_path.exists():
                        # é€‰æ‹©ä»£è¡¨æ€§æ¢¯åº¦çº§åˆ«
                        levels = [0, 25, 50, 75, 99]  # 5ä¸ªä¸åŒå¼ºåº¦
                        for level in levels:
                            img_file = gradient_path / f"gradient_{level:03d}.png"
                            if img_file.exists() and len(samples) < target_count:
                                samples.append({
                                    "image_path": str(img_file),
                                    "illusion_type": illusion_dir.name,
                                    "gradient_level": level,
                                    "intensity": level / 99.0,
                                    "description": f"{illusion_dir.name} - gradient level {level}",
                                    "difficulty": "easy" if level < 30 else "hard"
                                })
        
        return samples[:target_count]

    def copy_samples_to_category(self, samples, target_path):
        """å¤åˆ¶æ ·æœ¬åˆ°ç›®æ ‡ç±»åˆ«"""
        if not samples:
            return
            
        images_path = target_path / "images"
        metadata_path = target_path / "metadata"
        
        for i, sample in enumerate(samples):
            if "image_path" in sample and Path(sample["image_path"]).exists():
                # å¤åˆ¶å›¾ç‰‡æ–‡ä»¶
                src_img = Path(sample["image_path"])
                dst_img = images_path / f"sample_{i+1:03d}{src_img.suffix}"
                
                try:
                    shutil.copy2(src_img, dst_img)
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    sample["final_image_path"] = str(dst_img)
                    with open(metadata_path / f"sample_{i+1:03d}.json", 'w', encoding='utf-8') as f:
                        json.dump(sample, f, indent=2, ensure_ascii=False)
                except Exception as e:
                    print(f"    âš ï¸ å¤åˆ¶å¤±è´¥: {src_img.name} - {e}")

    def save_generated_samples(self, samples, target_path):
        """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬ä¿¡æ¯"""
        if not samples:
            return
            
        metadata_path = target_path / "metadata"
        
        # ä¿å­˜æ ·æœ¬ä¿¡æ¯åˆ°å…ƒæ•°æ®
        for i, sample in enumerate(samples):
            sample["sample_index"] = i + 1
            sample["status"] = "generated_template"
            
            with open(metadata_path / f"sample_{i+1:03d}.json", 'w', encoding='utf-8') as f:
                json.dump(sample, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç±»åˆ«æ€»ç»“
        category_summary = {
            "category": target_path.parent.name,
            "subcategory": target_path.name,
            "total_samples": len(samples),
            "status": "templates_created",
            "creation_date": datetime.now().isoformat()
        }
        
        with open(target_path / "category_summary.json", 'w', encoding='utf-8') as f:
            json.dump(category_summary, f, indent=2, ensure_ascii=False)

    def save_relation_samples(self, samples, target_path):
        """ä¿å­˜å…³ç³»æ ·æœ¬"""
        if not samples:
            return
            
        images_path = target_path / "images"
        metadata_path = target_path / "metadata"
        
        for i, sample in enumerate(samples):
            if "image_path" in sample and Path(sample["image_path"]).exists():
                # å¤åˆ¶å›¾ç‰‡
                src_img = Path(sample["image_path"])
                dst_img = images_path / f"relation_{i+1:03d}{src_img.suffix}"
                
                try:
                    shutil.copy2(src_img, dst_img)
                    sample["final_image_path"] = str(dst_img)
                except:
                    pass
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(metadata_path / f"relation_{i+1:03d}.json", 'w', encoding='utf-8') as f:
                json.dump(sample, f, indent=2, ensure_ascii=False)

    def generate_final_statistics(self):
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š...")
        
        total_samples = 0
        category_stats = {}
        
        for category_path in self.final_dataset_path.iterdir():
            if category_path.is_dir() and category_path.name in ["Subject", "Relation", "Attribute", "Illusion"]:
                category_samples = 0
                subcategory_stats = {}
                
                for subcat_path in category_path.iterdir():
                    if subcat_path.is_dir() and subcat_path.name not in ["images", "metadata"]:
                        # ç»Ÿè®¡å­ç±»åˆ«æ ·æœ¬æ•°
                        images_path = subcat_path / "images"
                        metadata_path = subcat_path / "metadata"
                        
                        image_count = len(list(images_path.glob("*"))) if images_path.exists() else 0
                        metadata_count = len(list(metadata_path.glob("*.json"))) if metadata_path.exists() else 0
                        
                        subcat_samples = max(image_count, metadata_count)
                        subcategory_stats[subcat_path.name] = subcat_samples
                        category_samples += subcat_samples
                
                category_stats[category_path.name] = {
                    "total_samples": category_samples,
                    "subcategories": subcategory_stats
                }
                total_samples += category_samples
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "dataset_name": "VLM Final Benchmark Dataset",
            "creation_date": datetime.now().isoformat(),
            "total_samples": total_samples,
            "categories": category_stats,
            "quality_standards": self.quality_standards,
            "summary": {
                "Subject": f"{category_stats.get('Subject', {}).get('total_samples', 0)} ä¸ªä¸»ä½“æ„ŸçŸ¥æ ·æœ¬",
                "Relation": f"{category_stats.get('Relation', {}).get('total_samples', 0)} ä¸ªå…³ç³»ç†è§£æ ·æœ¬",
                "Attribute": f"{category_stats.get('Attribute', {}).get('total_samples', 0)} ä¸ªå±æ€§æ„ŸçŸ¥æ ·æœ¬", 
                "Illusion": f"{category_stats.get('Illusion', {}).get('total_samples', 0)} ä¸ªé”™è§‰æ„ŸçŸ¥æ ·æœ¬"
            }
        }
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        with open(self.final_dataset_path / "final_dataset_statistics.json", 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        return final_report

    def create_final_readme(self, stats):
        """åˆ›å»ºæœ€ç»ˆREADME"""
        readme_content = f"""# VLM Final Benchmark Dataset

## ğŸ¯ æ•°æ®é›†æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ç»¼åˆåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œæ¶µç›–å››å¤§æ ¸å¿ƒè¯„ä¼°ç»´åº¦ã€‚

**åˆ›å»ºæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}  
**æ€»æ ·æœ¬æ•°**: {stats['total_samples']:,}  
**è´¨é‡æ ‡å‡†**: æ¯ç±»åˆ«è‡³å°‘50ä¸ªé«˜è´¨é‡æ ·æœ¬

## ğŸ“Š æ•°æ®é›†ç»Ÿè®¡

### å››å¤§ç±»åˆ«è¯¦æƒ…

| ç±»åˆ« | æ ·æœ¬æ•° | å­ç±»åˆ«æ•° | æè¿° |
|------|--------|---------|------|
| **Subject** | {stats['categories'].get('Subject', {}).get('total_samples', 0)} | {len(stats['categories'].get('Subject', {}).get('subcategories', {}))} | ä¸»ä½“æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° |
| **Relation** | {stats['categories'].get('Relation', {}).get('total_samples', 0)} | {len(stats['categories'].get('Relation', {}).get('subcategories', {}))} | å…³ç³»ç†è§£èƒ½åŠ›è¯„ä¼° |
| **Attribute** | {stats['categories'].get('Attribute', {}).get('total_samples', 0)} | {len(stats['categories'].get('Attribute', {}).get('subcategories', {}))} | å±æ€§æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° |
| **Illusion** | {stats['categories'].get('Illusion', {}).get('total_samples', 0)} | {len(stats['categories'].get('Illusion', {}).get('subcategories', {}))} | é”™è§‰æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼° |

### å­ç±»åˆ«è¯¦æƒ…

"""
        
        for category, info in stats['categories'].items():
            readme_content += f"""
#### {category}ç±»åˆ« ({info['total_samples']}ä¸ªæ ·æœ¬)
"""
            for subcat, count in info['subcategories'].items():
                readme_content += f"- **{subcat}**: {count} ä¸ªæ ·æœ¬\n"
        
        readme_content += f"""

## ğŸ“ æ•°æ®é›†ç»“æ„

```
VLM_Final_Benchmark_Dataset/
â”œâ”€â”€ Subject/                    # ä¸»ä½“æ„ŸçŸ¥ ({stats['categories'].get('Subject', {}).get('total_samples', 0)}ä¸ª)
â”‚   â”œâ”€â”€ clarity_degradation/    # æ¸…æ™°åº¦é€€åŒ–æ„ŸçŸ¥
â”‚   â”œâ”€â”€ brightness_variation/   # äº®åº¦å˜åŒ–æ„ŸçŸ¥  
â”‚   â”œâ”€â”€ contrast_variation/     # å¯¹æ¯”åº¦å˜åŒ–æ„ŸçŸ¥
â”‚   â”œâ”€â”€ color_distortion/      # é¢œè‰²å¤±çœŸæ„ŸçŸ¥
â”‚   â”œâ”€â”€ color_shift/           # è‰²åè¯†åˆ«
â”‚   â”œâ”€â”€ fine_grained_classification/ # ç»†ç²’åº¦åˆ†ç±»
â”‚   â””â”€â”€ resolution_variation/   # åˆ†è¾¨ç‡å˜åŒ–
â”‚
â”œâ”€â”€ Relation/                   # å…³ç³»ç†è§£ ({stats['categories'].get('Relation', {}).get('total_samples', 0)}ä¸ª)
â”‚   â”œâ”€â”€ spatial_relations/      # ç©ºé—´ä½ç½®å…³ç³»
â”‚   â”œâ”€â”€ proximity_relations/    # è·ç¦»/é è¿‘å…³ç³»
â”‚   â”œâ”€â”€ alignment_relations/    # å¯¹é½/æ–¹å‘å…³ç³»
â”‚   â””â”€â”€ comparative_relations/  # æ¯”è¾ƒå…³ç³»
â”‚
â”œâ”€â”€ Attribute/                  # å±æ€§æ„ŸçŸ¥ ({stats['categories'].get('Attribute', {}).get('total_samples', 0)}ä¸ª)
â”‚   â”œâ”€â”€ global_noise/          # å›¾åƒæ•´ä½“åŠ å™ªå£°
â”‚   â”œâ”€â”€ pixel_manipulation/    # åƒç´ ç‚¹æ“ä½œ
â”‚   â”œâ”€â”€ texture_analysis/      # çº¹ç†åˆ†æ
â”‚   â””â”€â”€ pattern_recognition/   # æ¨¡å¼è¯†åˆ«
â”‚
â””â”€â”€ Illusion/                   # é”™è§‰æ„ŸçŸ¥ ({stats['categories'].get('Illusion', {}).get('total_samples', 0)}ä¸ª)
    â”œâ”€â”€ geometric_illusions/    # å‡ ä½•é”™è§‰
    â”œâ”€â”€ color_illusions/       # è‰²å½©é”™è§‰
    â”œâ”€â”€ motion_illusions/      # è¿åŠ¨é”™è§‰
    â””â”€â”€ ambiguous_figures/     # æ¨¡ç³Šå›¾å½¢
```

## ğŸ¯ è´¨é‡æ ‡å‡†

- âœ… **æ ·æœ¬æ•°é‡**: æ¯ä¸ªå­ç±»åˆ«è‡³å°‘50ä¸ªæ ·æœ¬
- âœ… **å›¾ç‰‡è´¨é‡**: åˆ†è¾¨ç‡â‰¥512x512ï¼Œæ¸…æ™°åº¦è‰¯å¥½
- âœ… **æ•°æ®å¤šæ ·æ€§**: åœºæ™¯ä¸°å¯Œï¼Œè§’åº¦å¤šæ ·
- âœ… **æ ‡æ³¨å‡†ç¡®**: å®Œæ•´çš„å…ƒæ•°æ®å’Œæè¿°
- âœ… **éš¾åº¦åˆ†çº§**: Easy/Medium/Hardä¸‰ä¸ªéš¾åº¦çº§åˆ«

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### æ•°æ®åŠ è½½
```python
import json
from pathlib import Path

# åŠ è½½æ•°æ®é›†
dataset_path = Path("VLM_Final_Benchmark_Dataset")

# åŠ è½½Subjectç±»åˆ«
subject_path = dataset_path / "Subject" / "clarity_degradation"
images_path = subject_path / "images"
metadata_path = subject_path / "metadata"

# éå†æ ·æœ¬
for img_file in images_path.glob("*.png"):
    # åŠ è½½å¯¹åº”å…ƒæ•°æ®
    meta_file = metadata_path / f"{{img_file.stem}}.json"
    if meta_file.exists():
        with open(meta_file) as f:
            metadata = json.load(f)
        # å¤„ç†æ ·æœ¬...
```

### è¯„ä¼°æ¡†æ¶
```python
def evaluate_vlm_on_dataset(model, dataset_path):
    results = {{}}
    
    for category in ["Subject", "Relation", "Attribute", "Illusion"]:
        category_results = []
        category_path = dataset_path / category
        
        for subcat_path in category_path.iterdir():
            if subcat_path.is_dir():
                # è¯„ä¼°å­ç±»åˆ«
                subcat_results = evaluate_subcategory(model, subcat_path)
                category_results.extend(subcat_results)
        
        results[category] = category_results
    
    return results
```

## ğŸ“ˆ è¯„ä¼°ç»´åº¦

### ä¸»è¦è¯„ä¼°æŒ‡æ ‡
1. **å‡†ç¡®ç‡**: æ­£ç¡®è¯†åˆ«/æè¿°çš„æ¯”ä¾‹
2. **é²æ£’æ€§**: åœ¨é€€åŒ–æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¿æŒåº¦  
3. **ä¸€è‡´æ€§**: ç›¸ä¼¼æ¡ä»¶ä¸‹çš„è¾“å‡ºä¸€è‡´æ€§
4. **è§£é‡Šèƒ½åŠ›**: å¯¹ç°è±¡çš„è§£é‡Šåˆç†æ€§

### éš¾åº¦çº§åˆ«
- **Easy**: åŸºç¡€è¯†åˆ«å’Œæè¿°ä»»åŠ¡
- **Medium**: éœ€è¦ä¸€å®šæ¨ç†çš„ä»»åŠ¡
- **Hard**: å¤æ‚åœºæ™¯å’Œæç«¯æ¡ä»¶

## ğŸ‰ æ•°æ®é›†ç‰¹ç‚¹

1. **å…¨é¢è¦†ç›–**: å››å¤§æ ¸å¿ƒèƒ½åŠ›ç»´åº¦å…¨è¦†ç›–
2. **é«˜è´¨é‡**: ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶æ ‡å‡†
3. **çœŸå®æ€§**: åŸºäºçœŸå®ä¸–ç•Œåœºæ™¯å’Œç°è±¡
4. **å¯æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•
5. **æ ‡å‡†åŒ–**: ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œæ ‡æ³¨è§„èŒƒ

## ğŸ“ å¼•ç”¨

```bibtex
@dataset{{vlm_final_benchmark,
  title={{VLM Final Benchmark Dataset}},
  year={{2025}},
  month={{08}},
  note={{Comprehensive high-quality benchmark for Visual Language Models}}
}}
```

## ğŸ“„ è®¸å¯è¯

æœ¬æ•°æ®é›†ä»…ä¾›ç ”ç©¶ä½¿ç”¨ã€‚

---
*æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d")}*
*ç»´æŠ¤: VLMç ”ç©¶å›¢é˜Ÿ*
"""

        with open(self.final_dataset_path / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)

    def create_final_dataset(self):
        """åˆ›å»ºæœ€ç»ˆæ•°æ®é›†"""
        print("ğŸš€ å¼€å§‹åˆ›å»ºæœ€ç»ˆé«˜è´¨é‡æ•°æ®é›†")
        print("=" * 60)
        
        # 1. è®¾ç½®ç»“æ„
        self.setup_final_structure()
        
        # 2. ç²¾é€‰å„ç±»åˆ«æ•°æ®
        subject_count = self.curate_subject_data()
        relation_count = self.curate_relation_data()
        attribute_count = self.curate_attribute_data()  
        illusion_count = self.curate_illusion_data()
        
        total_count = subject_count + relation_count + attribute_count + illusion_count
        
        # 3. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self.generate_final_statistics()
        
        # 4. åˆ›å»ºREADME
        self.create_final_readme(stats)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æœ€ç»ˆé«˜è´¨é‡æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {self.final_dataset_path}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
        print(f"ğŸ“‹ è¯¦ç»†ç»Ÿè®¡: final_dataset_statistics.json")
        print(f"ğŸ“– ä½¿ç”¨è¯´æ˜: README.md")
        
        return stats

def main():
    creator = FinalDatasetCreator()
    final_stats = creator.create_final_dataset()
    
    print(f"\nğŸŒŸ æ­å–œï¼ä½ çš„VLMç»¼åˆåŸºå‡†æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼")
    print(f"åŒ…å« {final_stats['total_samples']} ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–å››å¤§è¯„ä¼°ç»´åº¦ã€‚")

if __name__ == "__main__":
    main()