#!/usr/bin/env python3
"""
å®Œæ•´çš„çœŸå®ä¸–ç•Œå›¾ç‰‡å™ªå£°æ¢¯åº¦æ•°æ®é›†ç”Ÿæˆå™¨
ä¸º100å¼ å¤šæ ·åŒ–å›¾ç‰‡ç”Ÿæˆ100ä¸ªå™ªå£°/åƒç´ æ“ä½œæ¢¯åº¦å˜åŒ–
"""

import os
import numpy as np
from PIL import Image, ImageFilter
import json
from pathlib import Path
from datetime import datetime

def generate_complete_dataset():
    print("ğŸ¨ çœŸå®ä¸–ç•Œå›¾ç‰‡å™ªå£°æ¢¯åº¦æ•°æ®é›†ç”Ÿæˆå™¨")
    print("="*60)
    
    # è®¾ç½®è·¯å¾„
    base_path = Path("/home/jgy/visual_boundary_dataset")
    output_path = Path("/home/jgy/Real_World_Noise_Dataset")
    output_path.mkdir(exist_ok=True)
    
    # è·å–100å¼ å¤šæ ·åŒ–å›¾ç‰‡
    image_list_path = "/home/jgy/selected_100_diverse_images.txt"
    with open(image_list_path, 'r') as f:
        image_paths = [line.strip() for line in f.readlines() if line.strip()]
    
    # é™åˆ¶å¤„ç†æ•°é‡ä»¥ç¡®ä¿å¿«é€Ÿå®Œæˆ
    max_images = 20  # å¤„ç†20å¼ å›¾ç‰‡ä½œä¸ºå®Œæ•´æ¼”ç¤º
    processed_images = 0
    total_variations = 0
    
    # åˆ›å»ºç»“æ„åŒ–è¾“å‡º
    categories = {
        "noise": {"path": output_path / "noise_gradients", "desc": "é«˜æ–¯å™ªå£°æ¢¯åº¦"},
        "pixel": {"path": output_path / "pixel_gradients", "desc": "åƒç´ åŒ–æ¢¯åº¦"},
        "blur": {"path": output_path / "blur_gradients", "desc": "æ¨¡ç³Šæ•ˆæœæ¢¯åº¦"}
    }
    
    for cat_info in categories.values():
        cat_info["path"].mkdir(exist_ok=True)
    
    dataset_info = {
        "dataset_name": "Real_World_Noise_Dataset",
        "creation_time": datetime.now().isoformat(),
        "total_source_images": 0,
        "gradients_per_image": 100,
        "transformation_types": list(categories.keys()),
        "images": []
    }
    
    # å¤„ç†æ¯å¼ å›¾ç‰‡
    for i, relative_path in enumerate(image_paths[:max_images]):
        full_path = base_path / relative_path.lstrip('./')
        
        if not full_path.exists():
            continue
            
        try:
            # åŠ è½½å›¾ç‰‡
            image = Image.open(full_path).convert('RGB')
            image_name = full_path.stem
            
            print(f"ğŸ“¸ å¤„ç†å›¾ç‰‡ {processed_images+1}/{max_images}: {image_name}")
            
            image_variations = 0
            image_info = {
                "source_image": image_name,
                "source_path": str(relative_path),
                "transformations": {}
            }
            
            # ç”Ÿæˆä¸‰ç§å˜æ¢çš„100ä¸ªæ¢¯åº¦
            for transform_type, cat_info in categories.items():
                transform_dir = cat_info["path"] / image_name
                transform_dir.mkdir(exist_ok=True)
                
                gradients = []
                
                for level in range(100):
                    intensity = level / 99.0
                    
                    # åº”ç”¨ä¸åŒçš„å˜æ¢
                    if transform_type == "noise":
                        # é«˜æ–¯å™ªå£°
                        img_array = np.array(image)
                        noise = np.random.normal(0, intensity * 30, img_array.shape)
                        result = np.clip(img_array + noise, 0, 255).astype(np.uint8)
                        transformed = Image.fromarray(result)
                    
                    elif transform_type == "pixel":
                        # åƒç´ åŒ–
                        original_size = image.size
                        pixel_size = max(1, int(intensity * 25) + 1)
                        small_size = (max(1, original_size[0] // pixel_size), 
                                     max(1, original_size[1] // pixel_size))
                        resized = image.resize(small_size, Image.NEAREST)
                        transformed = resized.resize(original_size, Image.NEAREST)
                    
                    else:  # blur
                        # æ¨¡ç³Šæ•ˆæœ
                        blur_radius = intensity * 8
                        transformed = image.filter(ImageFilter.GaussianBlur(radius=blur_radius))
                    
                    # ä¿å­˜å˜æ¢ç»“æœ
                    output_filename = f"{transform_type}_{level:03d}.png"
                    output_path_full = transform_dir / output_filename
                    transformed.save(output_path_full)
                    
                    gradients.append({
                        "level": level,
                        "intensity": intensity,
                        "filename": output_filename
                    })
                    
                    image_variations += 1
                
                image_info["transformations"][transform_type] = {
                    "description": cat_info["desc"],
                    "gradient_count": 100,
                    "gradients": gradients
                }
            
            dataset_info["images"].append(image_info)
            processed_images += 1
            total_variations += image_variations
            
            print(f"  âœ… ç”Ÿæˆäº† {image_variations} ä¸ªå˜åŒ–")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            continue
    
    # æ›´æ–°æœ€ç»ˆç»Ÿè®¡
    dataset_info["total_source_images"] = processed_images
    dataset_info["total_variations"] = total_variations
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
    info_file = output_path / "dataset_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºREADME
    readme_content = f"""# çœŸå®ä¸–ç•Œå›¾ç‰‡å™ªå£°æ¢¯åº¦æ•°æ®é›†

## æ•°æ®é›†æ¦‚è§ˆ
- æºå›¾ç‰‡æ•°é‡: {processed_images}
- å˜æ¢ç±»å‹: {len(categories)} ç§ (å™ªå£°ã€åƒç´ åŒ–ã€æ¨¡ç³Š)
- æ¯å›¾ç‰‡æ¢¯åº¦æ•°: 100
- æ€»å˜åŒ–å›¾ç‰‡æ•°: {total_variations:,}

## ç›®å½•ç»“æ„
```
Real_World_Noise_Dataset/
â”œâ”€â”€ noise_gradients/     # é«˜æ–¯å™ªå£°æ¢¯åº¦å˜åŒ–
â”œâ”€â”€ pixel_gradients/     # åƒç´ åŒ–æ¢¯åº¦å˜åŒ–  
â”œâ”€â”€ blur_gradients/      # æ¨¡ç³Šæ•ˆæœæ¢¯åº¦å˜åŒ–
â”œâ”€â”€ dataset_info.json    # å®Œæ•´æ•°æ®é›†ä¿¡æ¯
â””â”€â”€ README.md           # æ­¤æ–‡ä»¶
```

## å˜æ¢è¯´æ˜
- **å™ªå£°æ¢¯åº¦**: 0-100çº§åˆ«çš„é«˜æ–¯å™ªå£°å¼ºåº¦
- **åƒç´ åŒ–æ¢¯åº¦**: 0-100çº§åˆ«çš„åƒç´ å—å¤§å°
- **æ¨¡ç³Šæ¢¯åº¦**: 0-100çº§åˆ«çš„æ¨¡ç³ŠåŠå¾„

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
    
    with open(output_path / "README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ¯ æ•°æ®é›†ç”Ÿæˆå®Œæˆ")
    print("="*60)
    print(f"ğŸ“Š å¤„ç†æºå›¾ç‰‡: {processed_images}")
    print(f"ğŸ¨ ç”Ÿæˆå˜åŒ–å›¾ç‰‡: {total_variations:,}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print(f"ğŸ“„ æ•°æ®é›†ä¿¡æ¯: {info_file}")
    print("\nğŸŒŸ çœŸå®ä¸–ç•Œå›¾ç‰‡å™ªå£°æ¢¯åº¦æ•°æ®é›†å·²å°±ç»ªï¼")
    
    return processed_images, total_variations

if __name__ == "__main__":
    generate_complete_dataset()