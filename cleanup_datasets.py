#!/usr/bin/env python3
"""
æ¸…ç†æ•°æ®é›†ä¸­çš„ç©ºæ–‡ä»¶å¤¹å’Œä¸æ¸…æ™°çš„ç»“æ„
"""

import os
import shutil
from pathlib import Path

def cleanup_empty_directories(base_path):
    """é€’å½’åˆ é™¤ç©ºç›®å½•"""
    removed_dirs = []
    
    def remove_empty_dirs(path):
        if not path.is_dir():
            return False
        
        # å…ˆå¤„ç†å­ç›®å½•
        subdirs = [p for p in path.iterdir() if p.is_dir()]
        for subdir in subdirs:
            if remove_empty_dirs(subdir):
                removed_dirs.append(str(subdir))
        
        # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸ºç©º
        try:
            contents = list(path.iterdir())
            if not contents:
                path.rmdir()
                return True
        except:
            pass
        
        return False
    
    remove_empty_dirs(Path(base_path))
    return removed_dirs

def analyze_dataset_structure(dataset_path):
    """åˆ†ææ•°æ®é›†ç»“æ„"""
    path = Path(dataset_path)
    if not path.exists():
        return None
    
    analysis = {
        "name": path.name,
        "total_files": 0,
        "image_files": 0,
        "empty_dirs": [],
        "large_dirs": [],
        "structure": {}
    }
    
    for root, dirs, files in os.walk(path):
        root_path = Path(root)
        level = len(root_path.relative_to(path).parts)
        
        # ç»Ÿè®¡æ–‡ä»¶
        analysis["total_files"] += len(files)
        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]
        analysis["image_files"] += len(image_files)
        
        # è®°å½•ç©ºç›®å½•
        if not files and not dirs:
            analysis["empty_dirs"].append(str(root_path.relative_to(path)))
        
        # è®°å½•å¤§ç›®å½•ï¼ˆ>100ä¸ªæ–‡ä»¶ï¼‰
        if len(files) > 100:
            analysis["large_dirs"].append({
                "path": str(root_path.relative_to(path)),
                "file_count": len(files),
                "image_count": len(image_files)
            })
    
    return analysis

def cleanup_vlm_benchmark():
    """æ¸…ç†VLMåŸºå‡†æ•°æ®é›†"""
    print("ğŸ§¹ æ¸…ç†VLMæ•°æ®é›†ç»“æ„")
    print("=" * 50)
    
    base_path = Path("/home/jgy")
    datasets_to_cleanup = [
        "VLM_Comprehensive_Benchmark",
        "Real_World_Noise_Dataset", 
        "Unified_Illusion_Dataset",
        "visual_boundary_dataset"
    ]
    
    cleanup_report = {
        "cleanup_summary": {},
        "recommendations": []
    }
    
    for dataset in datasets_to_cleanup:
        dataset_path = base_path / dataset
        if dataset_path.exists():
            print(f"\nğŸ“ åˆ†æ {dataset}...")
            
            # åˆ†æç»“æ„
            analysis = analyze_dataset_structure(dataset_path)
            if analysis:
                print(f"  ğŸ“Š æ€»æ–‡ä»¶: {analysis['total_files']}")
                print(f"  ğŸ–¼ï¸  å›¾ç‰‡: {analysis['image_files']}")
                print(f"  ğŸ“‚ ç©ºç›®å½•: {len(analysis['empty_dirs'])}")
                
                # æ¸…ç†ç©ºç›®å½•
                removed = cleanup_empty_directories(dataset_path)
                if removed:
                    print(f"  ğŸ—‘ï¸  åˆ é™¤ç©ºç›®å½•: {len(removed)} ä¸ª")
                    for d in removed[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        print(f"     - {d}")
                    if len(removed) > 5:
                        print(f"     - ... è¿˜æœ‰ {len(removed)-5} ä¸ª")
                
                cleanup_report["cleanup_summary"][dataset] = {
                    "total_files": analysis["total_files"],
                    "image_files": analysis["image_files"],
                    "empty_dirs_removed": len(removed),
                    "large_dirs": analysis["large_dirs"]
                }
                
                # ç”Ÿæˆå»ºè®®
                if analysis["empty_dirs"]:
                    cleanup_report["recommendations"].append(
                        f"{dataset}: æ¸…ç†äº† {len(analysis['empty_dirs'])} ä¸ªç©ºç›®å½•"
                    )
    
    return cleanup_report

def organize_clear_structure():
    """é‡æ–°ç»„ç»‡ä¸ºæ¸…æ™°çš„ç»“æ„"""
    print("\nğŸ—ï¸  é‡æ–°ç»„ç»‡æ•°æ®é›†ç»“æ„...")
    
    benchmark_path = Path("/home/jgy/VLM_Comprehensive_Benchmark")
    
    # æ£€æŸ¥ç¬¦å·é“¾æ¥çš„æœ‰æ•ˆæ€§
    broken_links = []
    valid_links = []
    
    for link_path in benchmark_path.rglob("*"):
        if link_path.is_symlink():
            if not link_path.exists():
                broken_links.append(str(link_path))
            else:
                valid_links.append(str(link_path))
    
    print(f"  ğŸ”— æœ‰æ•ˆé“¾æ¥: {len(valid_links)}")
    print(f"  âŒ æŸåé“¾æ¥: {len(broken_links)}")
    
    # åˆ é™¤æŸåçš„é“¾æ¥
    for broken_link in broken_links:
        try:
            Path(broken_link).unlink()
            print(f"     åˆ é™¤æŸåé“¾æ¥: {Path(broken_link).name}")
        except:
            pass
    
    return {
        "valid_links": len(valid_links),
        "broken_links_removed": len(broken_links)
    }

def generate_clean_summary():
    """ç”Ÿæˆæ¸…ç†åçš„æ€»ç»“"""
    print("\nğŸ“‹ ç”Ÿæˆæ¸…ç†æ€»ç»“...")
    
    # é‡æ–°ç»Ÿè®¡æ¸…ç†åçš„æ•°æ®
    datasets = {
        "VLM_Comprehensive_Benchmark": "/home/jgy/VLM_Comprehensive_Benchmark",
        "Real_World_Noise_Dataset": "/home/jgy/Real_World_Noise_Dataset",
        "Unified_Illusion_Dataset": "/home/jgy/Unified_Illusion_Dataset",
        "visual_boundary_dataset": "/home/jgy/visual_boundary_dataset"
    }
    
    summary = {}
    
    for name, path in datasets.items():
        if Path(path).exists():
            analysis = analyze_dataset_structure(path)
            if analysis:
                summary[name] = {
                    "total_files": analysis["total_files"],
                    "image_files": analysis["image_files"],
                    "empty_dirs": len(analysis["empty_dirs"]),
                    "structure_clean": len(analysis["empty_dirs"]) == 0
                }
    
    # ä¿å­˜æ¸…ç†æŠ¥å‘Š
    import json
    with open("/home/jgy/dataset_cleanup_report.json", 'w') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    return summary

def main():
    print("ğŸš€ å¼€å§‹VLMæ•°æ®é›†æ¸…ç†")
    
    # 1. æ¸…ç†ç©ºç›®å½•å’Œæ— æ•ˆç»“æ„
    cleanup_report = cleanup_vlm_benchmark()
    
    # 2. æ•´ç†ç¬¦å·é“¾æ¥
    link_report = organize_clear_structure()
    
    # 3. ç”Ÿæˆæ¸…ç†åçš„æ€»ç»“
    final_summary = generate_clean_summary()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ•°æ®é›†æ¸…ç†å®Œæˆï¼")
    print("=" * 50)
    
    total_images = sum(info.get("image_files", 0) for info in final_summary.values())
    clean_datasets = sum(1 for info in final_summary.values() if info.get("structure_clean", False))
    
    print(f"ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_images:,}")
    print(f"ğŸ§¹ æ¸…ç†æ•°æ®é›†: {clean_datasets}/{len(final_summary)}")
    print(f"ğŸ”— æœ‰æ•ˆé“¾æ¥: {link_report['valid_links']}")
    print(f"âŒ åˆ é™¤æŸåé“¾æ¥: {link_report['broken_links_removed']}")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: /home/jgy/dataset_cleanup_report.json")

if __name__ == "__main__":
    main()